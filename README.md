# Bloomberg Competition Trading System

A production-ready quantitative trading framework built for the Bloomberg terminal competition.

## Features

âœ… **Complete Pipeline**: Data acquisition â†’ Feature engineering â†’ Strategy execution â†’ Performance calculation â†’ Experiment tracking

âš ï¸ **PositionSizer Deprecated**: Risk management is now handled by Portfolio Construction framework with centralized constraints

âœ… **MetaModel Strategy Combination**: Machine learning-based combination of multiple trading strategies (Ridge, Lasso, Dynamic)

âœ… **Hyperparameter Optimization**: Comprehensive optimization with Optuna for strategies, MetaModels, and ML models

âœ… **Multiple Strategy Types**: Dual Momentum, ML-based, Fama-French 5-factor, and custom strategies

âœ… **Team Collaboration**: Config-driven approach with unified CLI for different experiment types

âœ… **Advanced Experiment Tracking**: Automatic logging to Weights & Biases with comprehensive visualizations and attribution analysis

âœ… **Robust Data Pipeline**: YFinance and Fama-French data providers with retry logic, data validation, and error handling

âœ… **Professional Backtesting**: Time-weighted returns, realistic transaction costs, risk metrics, and benchmark-relative performance

âœ… **Model Persistence**: Save and load trained models using ModelRegistry with versioning and artifacts

## Portfolio Construction & Risk Management

**âš ï¸ IMPORTANT: PositionSizer Completely Removed**

The `PositionSizer` class has been **completely removed** from the system. Risk management and position sizing are now handled by the Portfolio Construction framework:

- **Box-based method**: Uses `BoxBasedPortfolioBuilder` with systematic box allocation and constraints
- **Quantitative method**: Uses `QuantitativePortfolioBuilder` with mathematical optimization and constraints

All risk controls (position limits, leverage, volatility targeting) are now configured under `portfolio_construction.constraints` in your YAML configs.

**Migration**: If you were using `position_sizing` configs, move those parameters to `portfolio_construction.constraints`.

### Example Configuration

```yaml
portfolio_construction:
  method: "box_based"  # or "quantitative"
  
  # Box-based configuration
  box_weights:
    method: "equal"
    dimensions:
      size: ["large", "mid"]
      style: ["growth", "value"]
  stocks_per_box: 3
  allocation_method: "equal"
  
  # Centralized constraints (replaces position_sizing)
  constraints:
    max_position_weight: 0.10
    max_leverage: 1.0
    min_position_weight: 0.02
    volatility_target: 0.15  # quantitative only
```

## Quick Start

### Stock Universe Management (CSV)

You can load the training symbol universe from a CSV file while keeping backward compatibility with inline symbols.

CSV requirements:

- Required: `ticker`
- Optional (used for filtering and ordering if present): `market_cap`|`market_cap_corrected`, `weight`, `sector`|`section`, `region`|`country_code`

Filters supported (all optional): `min_market_cap`, `min_weight`, `max_stocks`, `include_sectors`, `exclude_sectors`, `regions`.

Config (Option A):

```yaml
training_setup:
  parameters:
    universe:
      source: "csv"
      csv_path: "./data/universes/sp500_holdings.csv"
      filters:
        min_market_cap: 2.0
        max_stocks: 50
        exclude_sectors: ["Real Estate"]
    symbols: []  # leaves backward compatibility when empty
```

Example loader API:

```python
from src.trading_system.data.utils.universe_loader import load_universe_from_csv, load_symbols_from_config

symbols = load_universe_from_csv("./data/universes/sp500_holdings.csv", {"max_stocks": 50})
```

### 1. Environment Setup

```bash
# Install dependencies
poetry install

# Activate virtual environment
poetry shell
```

### 2. Configuration

Edit `configs/strategy_config.yaml` to customize:
- Strategy parameters (lookback periods, asset selection criteria)
- Asset universe (symbols to trade)
- Backtest settings (initial capital, transaction costs)
- Experiment configuration

### 3. Run the Strategy

```bash
# Test the system
python test_pipeline.py --skip-long-test

# Run a full backtest
python run_strategy.py --experiment-name "my_experiment"

# Run in test mode (shorter time period)
python run_strategy.py --test-mode
```

### 4. Unified Experiment Runner

The system now provides a unified CLI for running different types of experiments:

```bash
# === æ ‡å‡†ç«¯åˆ°ç«¯å®éªŒ (è®­ç»ƒ+å›æµ‹) ===
# è¿è¡ŒåŒ…å«æ¨¡å‹è®­ç»ƒå’Œç­–ç•¥å›æµ‹çš„å®Œæ•´æµç¨‹
poetry run python run_experiment.py experiment                          # ä½¿ç”¨é»˜è®¤é…ç½®
poetry run python run_experiment.py experiment --config configs/ml_strategy_config_new.yaml       # XGBoostç­–ç•¥
poetry run python run_experiment.py experiment -c configs/e2e_ff5_experiment.yaml              # Fama-French 5å› å­æ¨¡å‹
poetry run python run_experiment.py experiment -c configs/lstm_strategy_config.yaml             # LSTMç­–ç•¥

# === è¶…å‚æ•°ä¼˜åŒ– ===
# ä¼˜åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹çš„è¶…å‚æ•°
poetry run python run_experiment.py optimize --type ml --trials 50 --config configs/ml_strategy_config_new.yaml
poetry run python run_experiment.py optimize --type ml --trials 100 --sampler tpe --metric sharpe_ratio

# === MetaModelè®­ç»ƒ ===
# ç»„åˆå¤šä¸ªç­–ç•¥çš„å…ƒæ¨¡å‹
poetry run python run_experiment.py metamodel --method ridge --alpha 0.5
poetry run python run_experiment.py metamodel --config configs/metamodel_experiment_config.yaml
poetry run python run_experiment.py metamodel --method lasso --alpha 0.1 --strategies "DualMomentum,MLStrategy,FF5Strategy"
```

### 5. å…·ä½“ä½¿ç”¨åœºæ™¯æ¨è

æ ¹æ®ä½ çš„éœ€æ±‚ï¼Œé€‰æ‹©ä»¥ä¸‹å‘½ä»¤ï¼š

#### åœºæ™¯1ï¼šåªæƒ³è®­ç»ƒ+å›æµ‹ä¸€ä¸ªXGBoostç­–ç•¥
```bash
poetry run python run_experiment.py experiment -c configs/ml_strategy_config_new.yaml
```

#### åœºæ™¯2ï¼šè®­ç»ƒ+è¶…å‚æ•°ä¼˜åŒ–+å›æµ‹ (æ¨è)
```bash
poetry run python run_experiment.py optimize --type ml --trials 50 --config configs/ml_strategy_config_new.yaml
```

#### åœºæ™¯3ï¼šè®­ç»ƒFama-French 5å› å­æ¨¡å‹
```bash
poetry run python run_experiment.py experiment -c configs/e2e_ff5_experiment.yaml
```

#### åœºæ™¯4ï¼šè®­ç»ƒLSTMæ·±åº¦å­¦ä¹ ç­–ç•¥
```bash
poetry run python run_experiment.py experiment -c configs/lstm_strategy_config.yaml
```

#### åœºæ™¯5ï¼šç»„åˆå¤šä¸ªç­–ç•¥ (é«˜çº§)
```bash
poetry run python run_experiment.py metamodel --config configs/metamodel_experiment_config.yaml
```

#### åœºæ™¯6ï¼šFF5 + Box-Basedç»„åˆæ„å»º (ğŸ†• æ–°åŠŸèƒ½)
```bash
# å¿«é€Ÿæ¼”ç¤ºFF5æ¨¡å‹ä¸Box-Firstæ–¹æ³•ç»“åˆ
python run_ff5_box_experiment.py --demo

# å®Œæ•´FF5 + Box-Basedå®éªŒ
python run_ff5_box_experiment.py --config configs/ff5_box_based_experiment.yaml

# ä½¿ç”¨ç»Ÿä¸€å®éªŒè¿è¡Œå™¨
poetry run python run_experiment.py experiment -c configs/ff5_box_demo.yaml
```

## Architecture

```
src/trading_system/
â”œâ”€â”€ data/                    # Data providers (YFinance, Fama-French, StrategyDataCollector)
â”œâ”€â”€ strategies/              # Strategy implementations (Dual Momentum, ML, FF5, BaseStrategy)
â”œâ”€â”€ backtest/                # Backtest engine with performance calculation
â”œâ”€â”€ models/                  # Model training and serving infrastructure
â”‚   â”œâ”€â”€ base/               # BaseModel interface and ModelRegistry
â”‚   â”œâ”€â”€ training/           # TrainingPipeline and MetaModel training
â”‚   â””â”€â”€ finetune/           # Hyperparameter optimization with Optuna
â”œâ”€â”€ portfolio_construction/  # ğŸ†• Box-First portfolio construction framework
â”‚   â”œâ”€â”€ interfaces.py      # IPortfolioBuilder and supporting interfaces
â”‚   â”œâ”€â”€ box_based_builder.py # Box-First methodology implementation
â”‚   â”œâ”€â”€ quantitative_builder.py # Traditional optimization wrapper
â”‚   â”œâ”€â”€ factory.py         # PortfolioBuilderFactory for method selection
â”‚   â””â”€â”€ box_weight_manager.py # Box weight allocation strategies
â”œâ”€â”€ orchestration/           # System orchestration (SystemOrchestrator, MetaModel)
â”œâ”€â”€ feature_engineering/     # Technical indicators and feature pipelines
â”œâ”€â”€ utils/                   # Utilities (WandB logger, risk metrics, position sizing)
â””â”€â”€ experiment_orchestrator.py  # Unified experiment orchestration
```

## ğŸ†• Box-First Portfolio Construction

The system now includes a **Box-First portfolio construction framework** that solves the concentration problem in traditional optimization methods.

### ğŸ¯ Problem Solved
Traditional quantitative optimization often concentrates investments in few boxes (e.g., 80% in [large/growth/US/tech], 15% in [large/growth/US/finance]). The **Box-First methodology** ensures:

- âœ… **Systematic box coverage** - Every target box gets representation
- âœ… **Controlled diversification** - No concentration in few boxes
- âœ… **Flexible allocation** - Multiple weight strategies supported
- âœ… **Signal-driven selection** - Top stocks selected within each box

### ğŸ—ï¸ 4-Dimensional Box Structure
- **Size**: large, mid, small
- **Style**: growth, value
- **Region**: developed, emerging
- **Sector**: Technology, Financials, Healthcare, etc.

### ğŸ“¦ Usage Examples

```bash
# Quick demo
python run_ff5_box_experiment.py --demo

# Full experiment
python run_ff5_box_experiment.py --config configs/ff5_box_based_experiment.yaml

# Using unified runner
poetry run python run_experiment.py experiment -c configs/ff5_box_demo.yaml
```

### ğŸ”§ Configuration Example

```yaml
strategy:
  parameters:
    portfolio_construction:
      method: "box_based"
      stocks_per_box: 2
      allocation_method: "signal_proportional"
      box_weights:
        method: "equal"
        dimensions:
          size: ["large", "mid", "small"]
          style: ["growth", "value"]
          region: ["developed"]
          sector: ["Technology", "Financials", "Healthcare"]
```

### ğŸ“Š Key Benefits
| Feature | Box-Based | Traditional |
|---------|-----------|------------|
| Box Coverage | 60-80% | 10-30% |
| Concentration Risk | Low | High |
| Industry Diversification | High | Low |
| Sharpe Ratio Stability | More Stable | Variable |

ğŸ“– **Detailed Documentation**: See `FF5_BOX_README.md` for comprehensive usage guide.

## MetaModel Strategy Combination

The system includes a sophisticated MetaModel for combining multiple trading strategies:

### Features
- **Multiple Combination Methods**: Equal weighting, Lasso, Ridge regression, Dynamic optimization
- **Strategy Weight Learning**: Automatically learns optimal weights from historical performance
- **Cross-Validation**: Time series cross-validation with purge and embargo periods
- **Model Persistence**: Save and load trained MetaModels using ModelRegistry
- **Performance Attribution**: Analyze individual strategy contributions to combined performance

### Usage Examples

```bash
# Train a MetaModel with Ridge regression
poetry run python run_experiment.py metamodel --method ridge --alpha 0.5

# Train with custom strategy list and date range
poetry run python run_experiment.py metamodel \
  --method lasso \
  --alpha 0.1 \
  --strategies "DualMomentumStrategy,MLStrategy,FF5Strategy" \
  --start-date 2022-01-01 \
  --end-date 2023-12-31

# Use comprehensive YAML configuration
poetry run python run_experiment.py metamodel \
  --config configs/metamodel_experiment_config.yaml
```

### Configuration Example

```yaml
metamodel_training:
  method: "ridge"              # Combination method
  alpha: 0.5                   # Regularization strength
  strategies:                  # Strategies to combine
    - "DualMomentumStrategy"
    - "MLStrategy"
    - "FF5Strategy"
  start_date: "2022-01-01"
  end_date: "2023-12-31"
  use_cross_validation: true
  cv_folds: 5
```

## Hyperparameter Optimization

The system includes comprehensive hyperparameter optimization capabilities using Optuna with production-ready features:

### Features
- **Multiple Optimization Types**: MetaModel parameters, strategy parameters, ML model hyperparameters
- **Advanced Samplers**: TPE, Random, CMA-ES, Grid sampling algorithms
- **Pruning and Early Stopping**: Efficient search with median and hyperband pruning
- **Experiment Tracking**: Full integration with WandB for optimization trials
- **Parallel Optimization**: Multi-trial parallel execution
- **Model-Specific Search Spaces**: Pre-configured search spaces for XGBoost, LSTM, and FF5 models
- **Flexible Configuration**: YAML-based configuration with detailed parameter control

### Supported Model Types

#### XGBoost Models
**Hyperparameters Available:**
- `n_estimators`: [50, 500] - Number of trees
- `max_depth`: [3, 12] - Tree depth
- `learning_rate`: [0.01, 0.3] (log-scale) - Step size shrinkage
- `subsample`: [0.6, 1.0] - Sample ratio
- `colsample_bytree`: [0.6, 1.0] - Feature sampling ratio
- `reg_alpha`: [0.0, 1.0] - L1 regularization
- `reg_lambda`: [1.0, 5.0] - L2 regularization
- `min_child_weight`: [1, 10] - Minimum sum of instance weight

#### LSTM Models
**Hyperparameters Available:**
- `hidden_size`: [32, 64, 128, 256] - Hidden layer size
- `num_layers`: [1, 4] - Number of LSTM layers
- `dropout`: [0.1, 0.5] - Dropout rate
- `sequence_length`: [10, 20, 30, 60] - Input sequence length
- `learning_rate`: [0.001, 0.01] (log-scale) - Learning rate
- `batch_size`: [16, 32, 64] - Training batch size
- `num_epochs`: [50, 200] - Training epochs

#### Fama-French 5-Factor Models
**Hyperparameters Available:**
- `regularization`: [none, ridge] - Regularization method
- `alpha`: [0.01, 10.0] (log-scale) - Regularization strength
- `standardize`: [true, false] - Data standardization

### Usage Examples

```bash
# Optimize MetaModel parameters
poetry run python run_experiment.py optimize --type metamodel --trials 50

# Optimize strategy parameters with TPE sampler
poetry run python run_experiment.py optimize --type strategy --trials 100 --sampler tpe

# Optimize XGBoost model hyperparameters
poetry run python run_experiment.py optimize --type xgboost --trials 100 --metric sharpe_ratio

# Optimize LSTM model hyperparameters
poetry run python run_experiment.py optimize --type lstm --trials 50 --metric sortino_ratio

# Optimize FF5 model hyperparameters
poetry run python run_experiment.py optimize --type ff5 --trials 30 --metric r2

# Custom optimization with timeout and sampler
poetry run python run_experiment.py optimize \
  --type metamodel \
  --trials 100 \
  --timeout 300 \
  --sampler cmaes \
  --metric r2
```

### Configuration-Based Optimization

#### XGBoost Optimization Example
```yaml
xgboost_hyperparameter_optimization:
  enabled: true
  optimization_method: "optuna"
  n_trials: 100
  cv_folds: 5
  objective: "sharpe_ratio"
  direction: "maximize"

  sampler:
    type: "tpe"
    seed: 42

  pruner:
    type: "median"
    n_startup_trials: 5
    n_warmup_steps: 3

  search_space:
    preset: "xgboost_default"  # Use built-in preset

    # Custom parameters override preset
    custom_space:
      n_estimators:
        type: "int"
        low: 50
        high: 500
        step: 10
      max_depth:
        type: "int"
        low: 3
        high: 12
      learning_rate:
        type: "float"
        low: 0.01
        high: 0.3
        step: 0.01
        log_scale: true

  logging:
    log_optimization: true
    log_all_trials: true
    create_optimization_plot: true
```

#### LSTM Optimization Example
```yaml
lstm_hyperparameter_optimization:
  enabled: true
  optimization_method: "optuna"
  n_trials: 50
  cv_folds: 3
  objective: "sharpe_ratio"
  direction: "maximize"

  sampler:
    type: "tpe"
    seed: 42

  pruner:
    type: "median"
    n_startup_trials: 5
    n_warmup_steps: 3

  search_space:
    preset: "lstm_default"

    custom_space:
      hidden_size:
        type: "categorical"
        choices: [32, 64, 128, 256]
      num_layers:
        type: "int"
        low: 1
        high: 4
        step: 1
      dropout:
        type: "float"
        low: 0.1
        high: 0.5
        step: 0.05
      sequence_length:
        type: "categorical"
        choices: [10, 20, 30, 60]
```

#### FF5 Optimization Example
```yaml
ff5_hyperparameter_optimization:
  enabled: true
  optimization_method: "optuna"
  n_trials: 30
  cv_folds: 3
  objective: "r2"
  direction: "maximize"

  sampler:
    type: "tpe"
    seed: 42

  pruner:
    type: "median"
    n_startup_trials: 5
    n_warmup_steps: 3

  search_space:
    preset: "ff5_default"

    custom_space:
      regularization:
        type: "categorical"
        choices: ["none", "ridge"]
      alpha:
        type: "float"
        low: 0.01
        high: 10.0
        step: 0.1
        log_scale: true
      standardize:
        type: "categorical"
        choices: [true, false]
```

### Search Space Configuration

#### Parameter Types
- **`int`**: Integer parameters with `low`, `high`, and optional `step`
- **`float`**: Float parameters with `low`, `high`, optional `step` and `log_scale`
- **`categorical`**: Discrete parameters with `choices` array
- **`bool`**: Boolean parameters (true/false)

#### Preset Search Spaces
The system provides built-in presets for common use cases:

- **`xgboost_default`**: Comprehensive XGBoost parameter search (120 trials recommended)
- **`xgboost_fast`**: Reduced search space for quick testing (50 trials recommended)
- **`lstm_default`**: Full LSTM architecture search (120 trials recommended)
- **`lstm_fast`**: Simplified LSTM search (50 trials recommended)
- **`lstm_deep`**: Deep LSTM architectures (150 trials recommended)
- **`ff5_default`**: FF5 factor model optimization (30 trials recommended)

### Optimization Objectives

#### Available Metrics
- **Performance Metrics**: `sharpe_ratio`, `sortino_ratio`, `calmar_ratio`, `information_ratio`
- **Risk Metrics**: `max_drawdown`, `volatility`, `var_95`
- **Statistical Metrics**: `r2`, `mse`, `mae`, `ic` (Information Coefficient)
- **Custom Metrics**: Any metric that can be calculated from backtest results

#### Direction
- **`maximize`**: Higher values are better (e.g., Sharpe ratio)
- **`minimize`**: Lower values are better (e.g., MSE, maximum drawdown)

### Advanced Features

#### Cross-Validation Integration
- **Time Series Split**: Proper time-aware cross-validation
- **Purge Period**: Days to purge between train and test
- **Embargo Period**: Days to embargo before test period

#### Early Stopping
- **Median Pruning**: Stops trials that perform worse than median
- **Hyperband Pruning**: Asynchronous successive halving
- **Successive Halving**: Budget allocation to promising trials

#### Logging and Visualization
- **WandB Integration**: Automatic logging of all trials
- **Optimization Plots**: Interactive optimization history
- **Parameter Importance**: Most influential parameters
- **Parallel Coordinates**: Multi-dimensional parameter relationships

### Best Practices

1. **Trial Count**: Start with 30-50 trials for exploration, then 100-200 for refinement
2. **Sampler Choice**: Use TPE for most cases, Random for exploration, CMA-ES for complex spaces
3. **Metric Selection**: Choose metrics appropriate to your strategy goals
4. **Cross-Validation**: Use at least 3-fold CV, more for noisy strategies
5. **Early Stopping**: Enable pruning to save computation time
6. **Log Analysis**: Analyze optimization results to understand parameter importance

### Model-Specific Considerations

#### XGBoost
- Use `log_scale: true` for learning rate and regularization parameters
- Limit tree depth to prevent overfitting
- Consider `subsample` and `colsample_bytree` for robustness

#### LSTM
- Start with smaller architectures (1-2 layers)
- Use dropout to prevent overfitting
- Sequence length should match your data frequency
- Monitor for vanishing/exploding gradients

#### FF5
- Factor models typically have fewer tunable parameters
- Focus on regularization strength
- Standardization can significantly impact performance

## Key Components

### YFinanceProvider
- Automatic retry on API failures
- Data validation and cleaning
- Rate limiting and error handling
- Support for multiple data types

### DualMomentumStrategy
- Absolute momentum filter (positive returns only)
- Relative momentum selection (top performers)
- Equal-weight allocation
- Cash position for risk management

### StandardBacktest
- Time-weighted returns calculation
- Comprehensive risk metrics
- Transaction cost modeling
- Benchmark-relative performance

### WandBLogger
- Automatic experiment tracking
- Interactive visualizations
- Hyperparameter logging
- Team collaboration features

## Configuration Example

```yaml
strategy:
  lookback_days: 252          # 1 year momentum
  top_n_assets: 5             # Select top 5 assets
  minimum_positive_assets: 3  # Stay invested only if 3+ assets positive

universe:
  all_assets:
    - "SPY"    # S&P 500
    - "QQQ"    # Nasdaq 100
    - "IWM"    # Russell 2000
    - "AGG"    # Bonds
    - "TLT"    # Long-term Treasury

backtest:
  initial_capital: 1000000    # $1M starting capital
  transaction_cost: 0.001     # 0.1% per trade
  start_date: "2018-01-01"
  end_date: "2024-12-31"
```

## Environment Variables

Required:
```bash
WANDB_API_KEY=your_wandb_key          # From wandb.ai
ALPHA_VANTAGE_API_KEY=your_av_key     # Backup data source
```

## Testing

```bash
# Run all tests
python test_pipeline.py

# Skip long-running tests
python test_pipeline.py --skip-long-test

# Test specific components
poetry run python -c "
from trading_system.data.yfinance_provider import YFinanceProvider
provider = YFinanceProvider()
print('SPY valid:', provider.validate_symbol('SPY'))
"
```

## First Week Milestones âœ…

**Day 1-3: Core Infrastructure**
- [x] Poetry project setup
- [x] YFinance data provider with error handling
- [x] Strategy interface and dual momentum implementation
- [x] Basic backtest engine
- [x] WandB integration

**Day 4-5: Pipeline Integration**
- [x] Configuration management
- [x] Strategy runner orchestration
- [x] End-to-end pipeline testing
- [x] Performance metrics calculation

**Day 6-7: Team Readiness**
- [x] Complete pipeline validation
- [x] Team-friendly configuration system
- [x] Documentation and examples
- [x] Test coverage for all components

## Team Usage

### For Non-Programmers
1. **Basic Strategy Testing**: Modify `configs/strategy_config.yaml` to change parameters
2. **Run Experiments**: Use the unified CLI for different experiment types:
   ```bash
   poetry run python run_experiment.py --config my_config.yaml
   poetry run python run_experiment.py metamodel --method ridge --alpha 0.5
   ```
3. **Hyperparameter Optimization**: Find optimal parameters automatically:
   ```bash
   poetry run python run_experiment.py optimize --type strategy --trials 50
   ```
4. **View Results**: Check results in `./results/` folder and WandB dashboard

### For Developers
1. **Strategy Development**: Implement new strategies by extending `BaseStrategy`
2. **MetaModel Development**: Create new combination methods in `MetaModel` class
3. **Data Integration**: Add new data providers implementing the same interface
4. **Optimization**: Define custom search spaces for new parameters
5. **Performance Analysis**: Extend performance metrics and attribution analysis
6. **Model Persistence**: Use ModelRegistry for saving/loading trained models

### For Quant Researchers
1. **Strategy Combination**: Use MetaModel to combine multiple strategies
2. **Parameter Optimization**: Use hyperparameter optimization for robustness testing
3. **Performance Attribution**: Analyze strategy contributions and risk factors
4. **Backtesting**: Use comprehensive backtesting with realistic transaction costs

## Performance Metrics

The system calculates:
- Total and annualized returns
- Volatility and Sharpe ratio
- Maximum drawdown and Calmar ratio
- Alpha and beta vs benchmark
- Information ratio and tracking error
- Win rate and profit factor
- Turnover and concentration risk

## Next Steps

1. **Add More Strategies**: Implement mean-reversion, value, or carry strategies
2. **Parameter Optimization**: Integrate Optuna for hyperparameter tuning
3. **Risk Management**: Add volatility targeting, stop-losses
4. **Alternative Data**: Add Alpha Vantage as backup data source
5. **Real-time Trading**: Extend for live trading capabilities

## Contributing

1. Fork the repository
2. Create a feature branch
3. Implement your strategy extending `BaseStrategy`
4. Add tests for your implementation
5. Run the test suite
6. Submit a pull request

## License

This project is for the Bloomberg competition. Please ensure compliance with competition rules.