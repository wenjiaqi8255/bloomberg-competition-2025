# LSTM Strategy Configuration
# ==========================
# This configuration demonstrates LSTM neural network strategy setup with feature engineering.

# Part 1: Data Provider Configuration
# ---------------------------------
data_provider:
  type: "YFinanceProvider"
  parameters:
    max_retries: 3
    retry_delay: 1.0

# Part 2: Training Pipeline Configuration
# ------------------------------------
training_setup:
  model:
    model_type: "lstm"  # Use LSTM model for neural network-based strategy
    config:
      # LSTM architecture parameters - simplified for memory efficiency
      sequence_length: 10      # Reduced number of time steps to look back
      hidden_size: 32          # Reduced number of hidden units
      num_layers: 1            # Single LSTM layer to reduce complexity
      dropout: 0.1             # Lower dropout rate
      bidirectional: false     # Keep unidirectional for efficiency

      # Training parameters - simplified
      learning_rate: 0.01      # Higher learning rate for faster convergence
      batch_size: 16           # Smaller batch size to reduce memory usage
      epochs: 20               # Reduced number of epochs for quick test
      early_stopping_patience: 5   # Reduced patience

      # Device configuration
      device: "cpu"            # Force CPU to avoid GPU memory issues

  feature_engineering:
    enabled_features: ['momentum', 'volatility']  # Reduced feature set
    momentum_periods: [21]        # Single momentum period
    volatility_windows: [20]      # Single volatility window
    lookback_periods: [20]        # Single lookback period
    min_ic_threshold: 0.02
    min_significance: 0.1
    feature_lag: 1
    include_technical: false      # Disable complex technical indicators

  # Parameters for the training pipeline execution
  parameters:
    start_date: "2018-01-01"
    end_date: "2019-12-31"
    symbols:
      - AAPL
      - MSFT
      - GOOGL
      - AMZN
      - META
      - TSLA
      - NVDA
      - JPM
      - V
      - WMT

  # Hyperparameter optimization configuration
  hyperparameter_optimization:
    enabled: true
    optimization_method: "optuna"
    n_trials: 50
    cv_folds: 3
    objective: "sharpe_ratio"

    # Search space using LSTM model defaults
    search_space_preset: "lstm_default"

    # Optuna sampler and pruner settings
    sampler_type: "tpe"
    pruner_type: "median"

    # Training optimization
    enable_early_stopping: true
    early_stopping_patience: 10

    # Logging
    log_to_wandb: true
    log_all_trials: true

# Part 3: Strategy Configuration
# ------------------------------
strategy:
  type: ml
  name: LSTMStrategy_v1

  # Model configuration
  model_id: placeholder_model_id  # Will be overwritten by orchestrator
  min_signal_strength: 0.1       # Minimum signal strength threshold

# Model hyperparameter optimization settings
hyperparameter_optimization:
  # Enable/disable hyperparameter optimization
  enabled: true

  # Optimization method (optuna, grid_search, random_search)
  optimization_method: "optuna"

  # Number of optimization trials
  n_trials: 50

  # Optimization timeout (in seconds) - alternative to n_trials
  timeout: null  # e.g., 3600 for 1 hour

  # Cross-validation settings
  cv_folds: 3
  purge_days: 10  # Days to purge after train period
  embargo_days: 5  # Days to embargo before test period

  # Optimization objective (sharpe_ratio, sortino_ratio, max_drawdown, total_return, r2, mse)
  objective: "sharpe_ratio"
  direction: "maximize"  # maximize or minimize

  # Sampler settings (optuna specific)
  sampler:
    type: "tpe"  # tpe, random, cmaes, grid
    seed: 42

  # Pruner settings (for early stopping of bad trials)
  pruner:
    type: "median"  # median, hyperband, successional_halving
    n_startup_trials: 5  # Number of trials before pruning starts
    n_warmup_steps: 10  # Number of steps before pruning starts
    interval_steps: 1  # Interval between pruning checks

  # Search space configuration (uses model defaults if not specified)
  search_space:
    # Use preset search space from SearchSpaceBuilder
    preset: "lstm_default"  # lstm_default, lstm_fast, lstm_deep, or custom

    # Custom search space (if preset is "custom")
    custom_space:
      hidden_size:
        type: "categorical"
        choices: [32, 64, 128, 256]
      num_layers:
        type: "int"
        low: 1
        high: 4
      dropout:
        type: "float"
        low: 0.1
        high: 0.5
        step: 0.05
      learning_rate:
        type: "float"
        low: 0.001
        high: 0.1
        log_scale: true
      sequence_length:
        type: "categorical"
        choices: [10, 20, 30, 60]
      batch_size:
        type: "categorical"
        choices: [16, 32, 64, 128]

  # Early stopping settings
  early_stopping:
    enabled: true
    patience: 10  # Number of epochs without improvement
    min_delta: 0.001  # Minimum change to qualify as improvement
    monitor: "val_loss"  # Metric to monitor for early stopping

  # Trial logging and visualization
  logging:
    log_optimization: true  # Log optimization progress to wandb
    log_all_trials: true  # Log all trials (not just best)
    create_optimization_plot: true  # Create optimization history plots
    log_feature_importance: true  # Log feature importance from best model

# Part 4: Backtesting Configuration
# --------------------------------
backtest:
  name: "LSTM_Strategy_Backtest"
  start_date: "2020-01-01"
  end_date: "2023-12-31"
  initial_capital: 1000000
  benchmark_symbol: "SPY"
  commission_rate: 0.001
  slippage_rate: 0.0005
  rebalance_frequency: "weekly"
  position_limit: 0.10
  rebalance_threshold: 0.001  # Allow small trades

# Universe configuration
universe:
  - AAPL
  - MSFT
  - GOOGL
  - AMZN
  - META
  - TSLA
  - NVDA
  - JPM
  - V
  - WMT

# Model training (if needed)
model_training:
  train_start: "2018-01-01"
  train_end: "2019-12-31"
  validation_split: 0.2
  test_size: 0.1

# Investment Framework for Box-based classification and allocation
investment_framework:
  enabled: false  # Disable box framework for ML strategy

# Experiment tracking configuration
experiment:
  name: "LSTM_Strategy_Experiment"
  project: "bloomberg-competition"
  tags: ["lstm", "neural_network", "ml_strategy"]
  notes: "LSTM neural network strategy with technical indicators and feature engineering"