#!/usr/bin/env python3
"""
Integration Test Example - é›†æˆæµ‹è¯•ç¤ºä¾‹
========================================

ä¸€è¡Œä»£ç åŠŸèƒ½é›†æˆæµ‹è¯•éªŒè¯ã€‚

Usage:
    >>> python integration_test_example.py
    >>> python integration_test_example.py --verbose
    >>> python integration_test_example.py --component-only
"""

import argparse
import logging
import sys
import time
from pathlib import Path
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / "src"))

# å¯¼å…¥æ‰€æœ‰ç»„ä»¶è¿›è¡Œé›†æˆæµ‹è¯•
from trading_system.orchestration.optimal_system_orchestrator import (
    OptimalSystemOrchestrator, OptimalSystemConfig,
    create_optimal_system_orchestrator, quick_optimal_system
)
from trading_system.orchestration.components.optimal_model_selector import (
    OptimalModelSelector, ModelSelectionConfig,
    create_model_selector, quick_best_model_selection
)
from trading_system.orchestration.components.optimal_metamodel_selector import (
    OptimalMetaModelSelector, MetaModelSelectionConfig,
    create_metamodel_selector, quick_optimal_metamodel
)
from trading_system.orchestration.components.system_performance_evaluator import (
    SystemPerformanceEvaluator, SystemEvaluationConfig,
    create_system_evaluator, quick_system_evaluation
)
from trading_system.orchestration.utils.model_selection_utils import (
    optimize_single_model, find_best_model, quick_model_comparison
)
from trading_system.orchestration.utils.system_combination_utils import (
    combine_strategy_signals, evaluate_system_performance, build_optimal_system
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MockDataProvider:
    """æ¨¡æ‹Ÿæ•°æ®æä¾›å™¨ç”¨äºæµ‹è¯•."""

    @staticmethod
    def get_test_data() -> Dict[str, Any]:
        """è·å–æµ‹è¯•æ•°æ®."""
        import numpy as np
        import pandas as pd

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        dates = pd.date_range('2023-01-01', '2023-12-31', freq='D')
        symbols = ['AAPL', 'GOOGL', 'MSFT']

        # ç”Ÿæˆä»·æ ¼æ•°æ®
        price_data = {}
        signal_data = {}
        return_data = {}

        for i, symbol in enumerate(symbols):
            np.random.seed(42 + i)  # ç¡®ä¿å¯é‡å¤æ€§
            prices = 100 + np.cumsum(np.random.normal(0.001, 0.02, len(dates)))
            returns = np.diff(prices) / prices[:-1]
            returns = np.concatenate([[0], returns])
            signals = np.random.normal(0, 0.1, len(dates))

            price_data[symbol] = pd.Series(prices, index=dates)
            return_data[symbol] = pd.Series(returns, index=dates)
            signal_data[symbol] = pd.Series(signals, index=dates)

        return {
            'train_data': {
                'prices': pd.DataFrame(price_data),
                'returns': pd.DataFrame(return_data),
                'signals': pd.DataFrame(signal_data)
            },
            'test_data': {
                'prices': pd.DataFrame(price_data),
                'returns': pd.DataFrame(return_data),
                'signals': pd.DataFrame(signal_data)
            },
            'strategy_data': {
                'returns': pd.DataFrame(signal_data),
                'performance': {
                    symbol: {
                        'sharpe_ratio': np.random.uniform(0.5, 1.5),
                        'total_return': np.random.uniform(0.1, 0.3),
                        'volatility': np.random.uniform(0.1, 0.2),
                        'max_drawdown': np.random.uniform(-0.2, -0.05)
                    }
                    for symbol in symbols
                }
            },
            'benchmark_data': {
                'returns': pd.Series(
                    np.random.normal(0.0008, 0.015, len(dates)),
                    index=dates, name='SPY'
                )
            }
        }


class IntegrationTester:
    """é›†æˆæµ‹è¯•å™¨."""

    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.test_results = {}
        self.data_provider = MockDataProvider()

    def log(self, message: str):
        """æ‰“å°æ—¥å¿—."""
        if self.verbose:
            print(f"ğŸ” {message}")
        else:
            logger.info(message)

    def test_pure_functions(self) -> bool:
        """æµ‹è¯•çº¯å‡½æ•°å±‚."""
        self.log("æµ‹è¯•çº¯å‡½æ•°å±‚...")
        start_time = time.time()

        try:
            # è·å–æµ‹è¯•æ•°æ®
            data = self.data_provider.get_test_data()

            # æµ‹è¯•æ¨¡å‹é€‰æ‹©çº¯å‡½æ•°
            self.log("  æµ‹è¯•æ¨¡å‹é€‰æ‹©çº¯å‡½æ•°...")
            model_types = ['xgboost', 'lstm']

            # è¿™é‡Œéœ€è¦çœŸå®çš„æ¨¡å‹é€‰æ‹©å‡½æ•°ï¼Œæš‚æ—¶ç”¨æ¨¡æ‹Ÿç»“æœ
            model_results = [
                {'model_id': 'xgboost_1', 'financial_metrics': {'sharpe_ratio': 1.2}},
                {'model_id': 'lstm_1', 'financial_metrics': {'sharpe_ratio': 1.0}}
            ]

            best_model = find_best_model(
                [result['financial_metrics'] for result in model_results],
                'sharpe_ratio', 5
            )
            assert best_model is not None, "æœ€ä½³æ¨¡å‹é€‰æ‹©å¤±è´¥"

            # æµ‹è¯•ç³»ç»Ÿç»„åˆçº¯å‡½æ•°
            self.log("  æµ‹è¯•ç³»ç»Ÿç»„åˆçº¯å‡½æ•°...")
            strategy_returns = data['strategy_data']['returns']
            portfolio_returns = strategy_returns.mean(axis=1)

            system_performance = evaluate_system_performance(
                portfolio_returns,
                {col: strategy_returns[col] for col in strategy_returns.columns[:2]},
                data['benchmark_data']['returns']
            )
            assert 'portfolio_metrics' in system_performance, "ç³»ç»Ÿæ€§èƒ½è¯„ä¼°å¤±è´¥"

            # æµ‹è¯•æ„å»ºæœ€ä¼˜ç³»ç»Ÿ
            self.log("  æµ‹è¯•æ„å»ºæœ€ä¼˜ç³»ç»Ÿ...")
            system_result = build_optimal_system(
                {col: data['test_data']['signals'][col] for col in strategy_returns.columns[:2]},
                data['strategy_data']['performance']
            )
            assert 'performance' in system_result, "ç³»ç»Ÿæ„å»ºå¤±è´¥"

            duration = time.time() - start_time
            self.test_results['pure_functions'] = {
                'status': 'PASS',
                'duration': duration,
                'details': f"æµ‹è¯•äº† {len(model_types)} ä¸ªæ¨¡å‹ç±»å‹"
            }
            self.log(f"  âœ… çº¯å‡½æ•°å±‚æµ‹è¯•é€šè¿‡ ({duration:.2f}s)")
            return True

        except Exception as e:
            self.test_results['pure_functions'] = {
                'status': 'FAIL',
                'duration': time.time() - start_time,
                'error': str(e)
            }
            self.log(f"  âŒ çº¯å‡½æ•°å±‚æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_delegate_classes(self) -> bool:
        """æµ‹è¯•å§”æ‰˜ç±»."""
        self.log("æµ‹è¯•å§”æ‰˜ç±»...")
        start_time = time.time()

        try:
            # è·å–æµ‹è¯•æ•°æ®
            data = self.data_provider.get_test_data()

            # æµ‹è¯•æ¨¡å‹é€‰æ‹©å™¨
            self.log("  æµ‹è¯•OptimalModelSelector...")
            model_selector = create_model_selector(n_trials=2)
            assert model_selector is not None, "æ¨¡å‹é€‰æ‹©å™¨åˆ›å»ºå¤±è´¥"

            # æµ‹è¯•å…ƒæ¨¡å‹é€‰æ‹©å™¨
            self.log("  æµ‹è¯•OptimalMetaModelSelector...")
            metamodel_selector = create_metamodel_selector(n_trials=2)
            assert metamodel_selector is not None, "å…ƒæ¨¡å‹é€‰æ‹©å™¨åˆ›å»ºå¤±è´¥"

            # æµ‹è¯•ç³»ç»Ÿè¯„ä¼°å™¨
            self.log("  æµ‹è¯•SystemPerformanceEvaluator...")
            system_evaluator = create_system_evaluator()
            assert system_evaluator is not None, "ç³»ç»Ÿè¯„ä¼°å™¨åˆ›å»ºå¤±è´¥"

            duration = time.time() - start_time
            self.test_results['delegate_classes'] = {
                'status': 'PASS',
                'duration': duration,
                'details': "æˆåŠŸåˆ›å»ºæ‰€æœ‰å§”æ‰˜ç±»"
            }
            self.log(f"  âœ… å§”æ‰˜ç±»æµ‹è¯•é€šè¿‡ ({duration:.2f}s)")
            return True

        except Exception as e:
            self.test_results['delegate_classes'] = {
                'status': 'FAIL',
                'duration': time.time() - start_time,
                'error': str(e)
            }
            self.log(f"  âŒ å§”æ‰˜ç±»æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_orchestrator(self) -> bool:
        """æµ‹è¯•ä¸»åè°ƒå™¨."""
        self.log("æµ‹è¯•ä¸»åè°ƒå™¨...")
        start_time = time.time()

        try:
            # è·å–æµ‹è¯•æ•°æ®
            data = self.data_provider.get_test_data()

            # æµ‹è¯•åè°ƒå™¨åˆ›å»º
            self.log("  æµ‹è¯•OptimalSystemOrchestratoråˆ›å»º...")
            orchestrator = create_optimal_system_orchestrator(n_trials=2, save_results=True)
            assert orchestrator is not None, "åè°ƒå™¨åˆ›å»ºå¤±è´¥"

            # æµ‹è¯•ä¸€è¡Œä»£ç åŠŸèƒ½
            self.log("  æµ‹è¯•ä¸€è¡Œä»£ç å¿«é€Ÿç³»ç»Ÿ...")
            model_types = ['xgboost', 'lstm']

            # è¿™é‡Œç®€åŒ–æµ‹è¯•ï¼Œå®é™…ä½¿ç”¨ä¸­éœ€è¦çœŸå®æ•°æ®
            try:
                result = quick_optimal_system(
                    model_types, data['train_data'], data['test_data'],
                    data['strategy_data'], data['benchmark_data'], n_trials=1
                )
                assert 'success' in result, "å¿«é€Ÿç³»ç»Ÿè¿”å›æ ¼å¼é”™è¯¯"
                self.log("    âœ… å¿«é€Ÿç³»ç»Ÿæµ‹è¯•é€šè¿‡")
            except Exception as e:
                self.log(f"    âš ï¸ å¿«é€Ÿç³»ç»Ÿæµ‹è¯•è·³è¿‡ (éœ€è¦çœŸå®æ¨¡å‹): {e}")

            # æµ‹è¯•åè°ƒå™¨æ–¹æ³•
            self.log("  æµ‹è¯•åè°ƒå™¨æ ¸å¿ƒæ–¹æ³•...")
            config = OptimalSystemConfig(model_n_trials=1, metamodel_n_trials=1)
            test_orchestrator = OptimalSystemOrchestrator(config)
            assert test_orchestrator.config is not None, "åè°ƒå™¨é…ç½®å¤±è´¥"

            duration = time.time() - start_time
            self.test_results['orchestrator'] = {
                'status': 'PASS',
                'duration': duration,
                'details': "åè°ƒå™¨åˆ›å»ºå’ŒåŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡"
            }
            self.log(f"  âœ… ä¸»åè°ƒå™¨æµ‹è¯•é€šè¿‡ ({duration:.2f}s)")
            return True

        except Exception as e:
            self.test_results['orchestrator'] = {
                'status': 'FAIL',
                'duration': time.time() - start_time,
                'error': str(e)
            }
            self.log(f"  âŒ ä¸»åè°ƒå™¨æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_one_line_functionality(self) -> bool:
        """æµ‹è¯•ä¸€è¡Œä»£ç åŠŸèƒ½."""
        self.log("æµ‹è¯•ä¸€è¡Œä»£ç åŠŸèƒ½...")
        start_time = time.time()

        try:
            # æµ‹è¯•å„ç§ä¸€è¡Œä»£ç åˆ›å»ºå‡½æ•°
            self.log("  æµ‹è¯•ä¸€è¡Œä»£ç åˆ›å»ºå‡½æ•°...")

            # æµ‹è¯•æ¨¡å‹é€‰æ‹©å™¨åˆ›å»º
            selector1 = create_model_selector(n_trials=5)
            selector2 = create_model_selector(primary_metric='sortino_ratio')
            assert selector1.config.n_trials == 5, "æ¨¡å‹é€‰æ‹©å™¨é…ç½®å¤±è´¥"
            assert selector2.config.primary_metric == 'sortino_ratio', "æ¨¡å‹é€‰æ‹©å™¨æŒ‡æ ‡é…ç½®å¤±è´¥"

            # æµ‹è¯•å…ƒæ¨¡å‹é€‰æ‹©å™¨åˆ›å»º
            metamodel1 = create_metamodel_selector(n_trials=10, weight_method='equal')
            assert metamodel1.config.n_trials == 10, "å…ƒæ¨¡å‹é€‰æ‹©å™¨é…ç½®å¤±è´¥"
            assert metamodel1.config.weight_method == 'equal', "å…ƒæ¨¡å‹é€‰æ‹©å™¨æƒé‡æ–¹æ³•é…ç½®å¤±è´¥"

            # æµ‹è¯•ç³»ç»Ÿè¯„ä¼°å™¨åˆ›å»º
            evaluator1 = create_system_evaluator(primary_metrics=['sharpe_ratio'])
            evaluator2 = create_system_evaluator(min_requirements={'sharpe_ratio': 1.0})
            assert 'sharpe_ratio' in evaluator1.config.primary_metrics, "ç³»ç»Ÿè¯„ä¼°å™¨ä¸»è¦æŒ‡æ ‡é…ç½®å¤±è´¥"
            assert evaluator2.config.min_requirements['sharpe_ratio'] == 1.0, "ç³»ç»Ÿè¯„ä¼°å™¨æœ€ä½è¦æ±‚é…ç½®å¤±è´¥"

            # æµ‹è¯•åè°ƒå™¨åˆ›å»º
            orchestrator1 = create_optimal_system_orchestrator(n_trials=20)
            orchestrator2 = create_optimal_system_orchestrator(save_results=False)
            assert orchestrator1.config.model_n_trials == 20, "åè°ƒå™¨æ¨¡å‹è¯•éªŒæ¬¡æ•°é…ç½®å¤±è´¥"
            assert orchestrator2.config.save_results == False, "åè°ƒå™¨ä¿å­˜ç»“æœé…ç½®å¤±è´¥"

            duration = time.time() - start_time
            self.test_results['one_line_functionality'] = {
                'status': 'PASS',
                'duration': duration,
                'details': "æ‰€æœ‰ä¸€è¡Œä»£ç åˆ›å»ºå‡½æ•°æµ‹è¯•é€šè¿‡"
            }
            self.log(f"  âœ… ä¸€è¡Œä»£ç åŠŸèƒ½æµ‹è¯•é€šè¿‡ ({duration:.2f}s)")
            return True

        except Exception as e:
            self.test_results['one_line_functionality'] = {
                'status': 'FAIL',
                'duration': time.time() - start_time,
                'error': str(e)
            }
            self.log(f"  âŒ ä¸€è¡Œä»£ç åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_error_handling(self) -> bool:
        """æµ‹è¯•é”™è¯¯å¤„ç†."""
        self.log("æµ‹è¯•é”™è¯¯å¤„ç†...")
        start_time = time.time()

        try:
            # æµ‹è¯•æ— æ•ˆé…ç½®å¤„ç†
            self.log("  æµ‹è¯•æ— æ•ˆé…ç½®å¤„ç†...")

            # æµ‹è¯•ç©ºæ¨¡å‹åˆ—è¡¨
            config = OptimalSystemConfig(model_n_trials=0)
            orchestrator = OptimalSystemOrchestrator(config)
            assert orchestrator.config.model_n_trials == 0, "æ— æ•ˆé…ç½®å¤„ç†å¤±è´¥"

            # æµ‹è¯•ç©ºæ•°æ®
            evaluator = create_system_evaluator()
            empty_result = evaluator.evaluate_complete_system(
                pd.Series(), {}, None
            )
            assert isinstance(empty_result, dict), "ç©ºæ•°æ®å¤„ç†å¤±è´¥"

            # æµ‹è¯•æ— æ•ˆæŒ‡æ ‡
            try:
                invalid_selector = create_model_selector(primary_metric='invalid_metric')
                # åº”è¯¥èƒ½åˆ›å»ºï¼Œä½†ä½¿ç”¨æ—¶ä¼šæŠ¥é”™
            except Exception:
                # è¿™æ˜¯é¢„æœŸçš„
                pass

            duration = time.time() - start_time
            self.test_results['error_handling'] = {
                'status': 'PASS',
                'duration': duration,
                'details': "é”™è¯¯å¤„ç†æœºåˆ¶æ­£å¸¸"
            }
            self.log(f"  âœ… é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡ ({duration:.2f}s)")
            return True

        except Exception as e:
            self.test_results['error_handling'] = {
                'status': 'FAIL',
                'duration': time.time() - start_time,
                'error': str(e)
            }
            self.log(f"  âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
            return False

    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•."""
        print("ğŸ§ª å¼€å§‹é›†æˆæµ‹è¯•")
        print("=" * 50)

        test_functions = [
            self.test_pure_functions,
            self.test_delegate_classes,
            self.test_orchestrator,
            self.test_one_line_functionality,
            self.test_error_handling
        ]

        passed = 0
        total = len(test_functions)

        for test_func in test_functions:
            if test_func():
                passed += 1

        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        total_time = sum(result.get('duration', 0) for result in self.test_results.values())
        success_rate = (passed / total) * 100

        summary = {
            'total_tests': total,
            'passed_tests': passed,
            'failed_tests': total - passed,
            'success_rate': success_rate,
            'total_duration': total_time,
            'test_results': self.test_results
        }

        print("\nğŸ“Š é›†æˆæµ‹è¯•æŠ¥å‘Š")
        print("=" * 50)
        print(f"æ€»æµ‹è¯•æ•°: {total}")
        print(f"é€šè¿‡æµ‹è¯•: {passed}")
        print(f"å¤±è´¥æµ‹è¯•: {total - passed}")
        print(f"æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"æ€»è€—æ—¶: {total_time:.2f}s")

        for test_name, result in self.test_results.items():
            status = "âœ… PASS" if result['status'] == 'PASS' else "âŒ FAIL"
            duration = result.get('duration', 0)
            print(f"{status} {test_name}: {duration:.2f}s")
            if result['status'] == 'FAIL':
                print(f"      é”™è¯¯: {result.get('error', 'Unknown error')}")

        if success_rate >= 80:
            print("\nğŸ‰ é›†æˆæµ‹è¯•åŸºæœ¬é€šè¿‡ï¼ä¸€è¡Œä»£ç åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
        else:
            print("\nâš ï¸ é›†æˆæµ‹è¯•å­˜åœ¨é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ã€‚")

        return summary


def main():
    """ä¸»å‡½æ•°."""
    parser = argparse.ArgumentParser(description='Integration Test Example')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--component-only', action='store_true',
                       help='ä»…æµ‹è¯•ç»„ä»¶ï¼Œä¸æµ‹è¯•å®Œæ•´ç³»ç»Ÿ')

    args = parser.parse_args()

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = IntegrationTester(verbose=args.verbose)

    if args.component_only:
        # ä»…æµ‹è¯•ç»„ä»¶
        tests = [tester.test_pure_functions, tester.test_delegate_classes, tester.test_one_line_functionality]
        passed = 0
        for test in tests:
            if test():
                passed += 1
        print(f"\nç»„ä»¶æµ‹è¯•ç»“æœ: {passed}/{len(tests)} é€šè¿‡")
    else:
        # è¿è¡Œå®Œæ•´æµ‹è¯•
        tester.run_all_tests()

    print("\nğŸ“– ä½¿ç”¨è¯´æ˜:")
    print("  - è¿™äº›æµ‹è¯•éªŒè¯äº†ä¸€è¡Œä»£ç åŠŸèƒ½çš„æ­£ç¡®æ€§")
    print("  - å®é™…ä½¿ç”¨æ—¶éœ€è¦æä¾›çœŸå®çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®")
    print("  - å‚è€ƒ examples/simple_usage_example.py äº†è§£åŸºæœ¬ç”¨æ³•")
    print("  - å‚è€ƒ examples/optimal_system_demo.py äº†è§£å®Œæ•´ç”¨æ³•")


if __name__ == "__main__":
    main()