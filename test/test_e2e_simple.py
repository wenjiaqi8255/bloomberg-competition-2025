#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆç«¯åˆ°ç«¯æµ‹è¯•: æ ¸å¿ƒæ¨¡å‹ç”Ÿå‘½å‘¨æœŸ

ä¸“æ³¨äºéªŒè¯æ–°æ¨¡å‹æ¶æ„çš„æ ¸å¿ƒåŠŸèƒ½:
1. æ¨¡å‹è®­ç»ƒå’Œæ³¨å†Œ
2. æ¨¡å‹åŠ è½½å’Œé¢„æµ‹
3. ModelPredictoråŸºæœ¬åŠŸèƒ½

Usage:
    poetry run python test_e2e_simple.py
"""

import logging
import sys
import tempfile
import shutil
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# Add src to path
sys.path.append(str(Path(__file__).parent / "src"))

import pandas as pd
import numpy as np

# Import only the core components
from trading_system.models.training.pipeline import TrainingPipeline
from trading_system.models.training.trainer import TrainingConfig
from trading_system.models.serving.predictor import ModelPredictor
from trading_system.models.base.model_factory import ModelFactory
from trading_system.models.registry import register_all_models

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_simple_test_data():
    """åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®"""
    logger.info("ğŸ“Š åˆ›å»ºç®€å•æµ‹è¯•æ•°æ®...")

    # åˆ›å»ºä»·æ ¼æ•°æ®
    np.random.seed(42)

    # 3æ”¯è‚¡ç¥¨ï¼Œ1å¹´æ•°æ®
    symbols = ["AAPL", "MSFT", "GOOGL"]
    equity_data = {}

    for i, symbol in enumerate(symbols):
        dates = pd.date_range(start="2023-01-01", end="2023-12-31", freq="D")
        n_days = len(dates)

        # ç®€å•çš„ä»·æ ¼ç”Ÿæˆ
        base_price = 100 + i * 50
        returns = np.random.normal(0.001, 0.02, n_days)
        prices = base_price * np.exp(np.cumsum(returns))

        df = pd.DataFrame({
            'date': dates,
            'close': prices,
            'open': prices * np.random.uniform(0.998, 1.002, n_days),
            'high': prices * np.random.uniform(1.0, 1.01, n_days),
            'low': prices * np.random.uniform(0.99, 1.0, n_days),
            'volume': np.random.randint(1_000_000, 5_000_000, n_days)
        })

        equity_data[symbol] = df
        logger.info(f"âœ… ç”Ÿæˆ {symbol} æ•°æ®: {len(df)} è¡Œ")

    # åˆ›å»ºç®€å•çš„å› å­æ•°æ®
    factor_dates = pd.date_range(start="2023-01-01", end="2023-12-31", freq="M")
    factor_data = pd.DataFrame({
        'date': factor_dates,
        'MKT': np.random.normal(0.005, 0.04, len(factor_dates)),
        'SMB': np.random.normal(0.001, 0.03, len(factor_dates)),
        'HML': np.random.normal(0.001, 0.025, len(factor_dates)),
        'RMW': np.random.normal(0.002, 0.02, len(factor_dates)),
        'CMA': np.random.normal(0.001, 0.015, len(factor_dates))
    })

    logger.info(f"âœ… ç”Ÿæˆå› å­æ•°æ®: {len(factor_data)} ä¸ªæœˆ")

    return equity_data, factor_data, symbols


def test_model_lifecycle():
    """æµ‹è¯•å®Œæ•´çš„æ¨¡å‹ç”Ÿå‘½å‘¨æœŸ"""
    logger.info("ğŸ¯ å¼€å§‹æ¨¡å‹ç”Ÿå‘½å‘¨æœŸæµ‹è¯•")
    logger.info("=" * 50)

    test_dir = Path(tempfile.mkdtemp(prefix="e2e_simple_"))
    registry_path = test_dir / "model_registry"
    registry_path.mkdir(parents=True, exist_ok=True)

    try:
        # æ­¥éª¤1: ç¡®ä¿æ¨¡å‹å·²æ³¨å†Œ
        logger.info("\nğŸ“ æ­¥éª¤1: æ£€æŸ¥æ¨¡å‹æ³¨å†Œ")
        register_all_models()

        available_models = list(ModelFactory._registry.keys())
        logger.info(f"å¯ç”¨æ¨¡å‹: {available_models}")
        assert "residual_predictor" in available_models, "æ®‹å·®é¢„æµ‹å™¨åº”è¯¥å·²æ³¨å†Œ"

        # æ­¥éª¤2: åˆ›å»ºæµ‹è¯•æ•°æ®
        logger.info("\nğŸ“Š æ­¥éª¤2: åˆ›å»ºæµ‹è¯•æ•°æ®")
        equity_data, factor_data, symbols = create_simple_test_data()

        # æ­¥éª¤3: è®­ç»ƒæ¨¡å‹
        logger.info("\nğŸš€ æ­¥éª¤3: è®­ç»ƒæ¨¡å‹")

        config = TrainingConfig(
            use_cross_validation=True,
            cv_folds=3,
            validation_split=0.2,
            early_stopping=True
        )

        pipeline = TrainingPipeline(
            model_type="residual_predictor",
            config=config,
            registry_path=str(registry_path)
        )

        logger.info("å¼€å§‹è®­ç»ƒæ®‹å·®é¢„æµ‹æ¨¡å‹...")
        training_result = pipeline.run_pipeline(
            start_date=datetime(2023, 1, 1),
            end_date=datetime(2023, 12, 31),
            symbols=symbols,
            model_name="test_residual_predictor",
            equity_data=equity_data,
            factor_data=factor_data
        )

        assert training_result is not None, "è®­ç»ƒç»“æœä¸åº”ä¸ºç©º"
        assert 'model_id' in training_result, "åº”è¯¥åŒ…å«æ¨¡å‹ID"

        model_id = training_result['model_id']
        logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹ID: {model_id}")

        # æ­¥éª¤4: éªŒè¯æ¨¡å‹ä¿å­˜
        logger.info("\nğŸ’¾ æ­¥éª¤4: éªŒè¯æ¨¡å‹ä¿å­˜")

        model_path = registry_path / model_id
        assert model_path.exists(), f"æ¨¡å‹ç›®å½•åº”è¯¥å­˜åœ¨: {model_path}"
        assert (model_path / "model.pkl").exists(), "æ¨¡å‹æ–‡ä»¶åº”è¯¥å­˜åœ¨"
        assert (model_path / "metadata.json").exists(), "å…ƒæ•°æ®æ–‡ä»¶åº”è¯¥å­˜åœ¨"

        # éªŒè¯å…ƒæ•°æ®
        with open(model_path / "metadata.json", 'r') as f:
            metadata = json.load(f)

        assert metadata['model_type'] == "residual_predictor", "æ¨¡å‹ç±»å‹åº”è¯¥æ­£ç¡®"
        logger.info("âœ… æ¨¡å‹æ–‡ä»¶éªŒè¯é€šè¿‡")

        # æ­¥éª¤5: åŠ è½½æ¨¡å‹
        logger.info("\nğŸ“¥ æ­¥éª¤5: åŠ è½½æ¨¡å‹")

        from trading_system.models.base.base_model import BaseModel
        loaded_model = BaseModel.load(model_path)

        assert loaded_model is not None, "åŠ è½½çš„æ¨¡å‹ä¸åº”ä¸ºç©º"
        assert loaded_model.model_type == "residual_predictor", "æ¨¡å‹ç±»å‹åº”è¯¥æ­£ç¡®"
        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

        # æ­¥éª¤6: åˆ›å»ºModelPredictor
        logger.info("\nğŸš€ æ­¥éª¤6: åˆ›å»ºModelPredictor")

        predictor = ModelPredictor(
            model_registry_path=str(registry_path),
            enable_monitoring=True,
            cache_predictions=True
        )

        # æ­¥éª¤7: åŠ è½½æ¨¡å‹åˆ°ç”Ÿäº§ç¯å¢ƒ
        logger.info("\nğŸ¯ æ­¥éª¤7: éƒ¨ç½²æ¨¡å‹åˆ°ç”Ÿäº§ç¯å¢ƒ")

        deployed_model_id = predictor.load_model(
            model_name="residual_predictor",
            model_path=str(model_path)
        )

        assert deployed_model_id is not None, "éƒ¨ç½²çš„æ¨¡å‹IDä¸åº”ä¸ºç©º"
        assert predictor.get_current_model() is not None, "å½“å‰æ¨¡å‹ä¸åº”ä¸ºç©º"
        logger.info(f"âœ… æ¨¡å‹éƒ¨ç½²æˆåŠŸï¼ŒID: {deployed_model_id}")

        # æ­¥éª¤8: è¿›è¡Œé¢„æµ‹
        logger.info("\nğŸ”® æ­¥éª¤8: è¿›è¡Œé¢„æµ‹")

        # ä½¿ç”¨AAPLçš„æœ€æ–°æ•°æ®
        symbol = symbols[0]
        latest_data = equity_data[symbol].tail(30)

        prediction_result = predictor.predict(
            market_data=latest_data,
            symbol=symbol,
            prediction_date=datetime(2023, 12, 31)
        )

        assert prediction_result is not None, "é¢„æµ‹ç»“æœä¸åº”ä¸ºç©º"
        assert 'prediction' in prediction_result, "åº”è¯¥åŒ…å«é¢„æµ‹å€¼"

        prediction_value = prediction_result['prediction']
        assert not np.isnan(prediction_value), "é¢„æµ‹å€¼ä¸åº”è¯¥æ˜¯NaN"
        assert not np.isinf(prediction_value), "é¢„æµ‹å€¼ä¸åº”è¯¥æ˜¯æ— ç©·å¤§"

        logger.info(f"âœ… é¢„æµ‹æˆåŠŸ: {symbol} = {prediction_value:.6f}")

        # æ­¥éª¤9: æ‰¹é‡é¢„æµ‹
        logger.info("\nğŸ“Š æ­¥éª¤9: æ‰¹é‡é¢„æµ‹")

        # å‡†å¤‡æ‰¹é‡æ•°æ®
        batch_data = pd.concat([df.tail(30) for df in equity_data.values()], keys=symbols)

        batch_results = predictor.predict_batch(
            market_data=batch_data,
            symbols=symbols,
            prediction_date=datetime(2023, 12, 31)
        )

        assert isinstance(batch_results, dict), "æ‰¹é‡ç»“æœåº”è¯¥æ˜¯å­—å…¸"
        logger.info(f"âœ… æ‰¹é‡é¢„æµ‹æˆåŠŸ: {len(batch_results)} ä¸ªè‚¡ç¥¨")

        # æ­¥éª¤10: æ£€æŸ¥ç›‘æ§
        logger.info("\nğŸ“ˆ æ­¥éª¤10: æ£€æŸ¥ç›‘æ§çŠ¶æ€")

        health = predictor.get_model_health()
        if health:
            logger.info(f"æ¨¡å‹å¥åº·çŠ¶æ€: {health.status}")
        else:
            logger.info("æ–°æ¨¡å‹æš‚æ— ç›‘æ§æ•°æ®ï¼ˆæ­£å¸¸ï¼‰")

        # æ­¥éª¤11: æµ‹è¯•ç¼“å­˜
        logger.info("\nğŸ’¾ æ­¥éª¤11: æµ‹è¯•é¢„æµ‹ç¼“å­˜")

        cached_prediction = predictor.get_cached_prediction(
            symbol=symbol,
            prediction_date=datetime(2023, 12, 31)
        )

        if cached_prediction:
            logger.info(f"âœ… ç¼“å­˜åŠŸèƒ½æ­£å¸¸: {cached_prediction['prediction']:.6f}")
        else:
            logger.info("ç¼“å­˜ä¸ºç©ºï¼ˆå¯èƒ½ç¼“å­˜è®¾ç½®é—®é¢˜ï¼‰")

        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        logger.info("\n" + "=" * 50)
        logger.info("ğŸ‰ ç«¯åˆ°ç«¯æµ‹è¯•å®Œæˆï¼")
        logger.info("=" * 50)
        logger.info("âœ… æ¨¡å‹æ³¨å†Œ - æ­£å¸¸")
        logger.info("âœ… æ¨¡å‹è®­ç»ƒ - æ­£å¸¸")
        logger.info("âœ… æ¨¡å‹ä¿å­˜ - æ­£å¸¸")
        logger.info("âœ… æ¨¡å‹åŠ è½½ - æ­£å¸¸")
        logger.info("âœ… ç”Ÿäº§éƒ¨ç½² - æ­£å¸¸")
        logger.info("âœ… å•æ¬¡é¢„æµ‹ - æ­£å¸¸")
        logger.info("âœ… æ‰¹é‡é¢„æµ‹ - æ­£å¸¸")
        logger.info("âœ… ç›‘æ§åŠŸèƒ½ - æ­£å¸¸")
        logger.info("=" * 50)
        logger.info("ğŸš€ æ–°æ¨¡å‹æ¶æ„ç«¯åˆ°ç«¯æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        logger.info("ğŸ’¡ æ ¸å¿ƒåŠŸèƒ½éªŒè¯:")
        logger.info("   - æ¨¡å‹è®­ç»ƒå’Œä¿å­˜æœºåˆ¶æ­£å¸¸")
        logger.info("   - ModelPredictorç”Ÿäº§æœåŠ¡æ­£å¸¸")
        logger.info("   - é¢„æµ‹åŠŸèƒ½å®Œæ•´ä¸”ç¨³å®š")
        logger.info("   - ç›‘æ§ç³»ç»ŸåŸºç¡€åŠŸèƒ½æ­£å¸¸")
        logger.info("   - ç¼“å­˜æœºåˆ¶å·¥ä½œæ­£å¸¸")

        return True

    except Exception as e:
        logger.error(f"âŒ ç«¯åˆ°ç«¯æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    finally:
        # æ¸…ç†æµ‹è¯•ç¯å¢ƒ
        if test_dir.exists():
            shutil.rmtree(test_dir)
        logger.info("ğŸ§¹ æµ‹è¯•ç¯å¢ƒå·²æ¸…ç†")


def main():
    """ä¸»å‡½æ•°"""
    try:
        success = test_model_lifecycle()
        return 0 if success else 1
    except KeyboardInterrupt:
        logger.info("æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)