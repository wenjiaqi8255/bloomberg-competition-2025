"""
Simple Hyperparameter Optimizer - MVP Version

æç®€çš„è¶…å‚æ•°ä¼˜åŒ–å™¨ï¼Œä¸“æ³¨æ ¸å¿ƒåŠŸèƒ½ï¼š
1. TPEä¼˜åŒ–
2. åŸºæœ¬è¯•éªŒç®¡ç†
3. ç»“æœä¿å­˜

No over-engineering, just essential functionality.
"""

import logging
from typing import Dict, Any, Optional, Callable
from dataclasses import dataclass
from pathlib import Path

import pandas as pd
import numpy as np

# Optuna imports
try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False

logger = logging.getLogger(__name__)


@dataclass
class HPOConfig:
    """Minimal HPO configuration."""
    n_trials: int = 50
    metric: str = "r2"
    direction: str = "maximize"
    save_dir: str = "./hpo_results"


class HyperparameterOptimizer:
    """
    æç®€è¶…å‚æ•°ä¼˜åŒ–å™¨ï¼Œä¸“æ³¨æ ¸å¿ƒåŠŸèƒ½ã€‚

    è®¾è®¡åŸåˆ™ï¼š
    - å•ä¸€èŒè´£ï¼šåªåšTPEä¼˜åŒ–
    - æç®€é…ç½®ï¼šæœ€å°‘å‚æ•°
    - ä¸€è¡Œä»£ç ï¼šèƒ½ç”¨ä¸€è¡Œè§£å†³çš„ç»ä¸ç”¨ä¸‰è¡Œ
    - æ— è¿‡åº¦è®¾è®¡ï¼šåˆ é™¤æ‰€æœ‰éå¿…éœ€åŠŸèƒ½
    """

    def __init__(self, n_trials: int = 50, metric: str = "r2", direction: str = "maximize",
                 model_train_func: Optional[Callable] = None):
        """
        æç®€åˆå§‹åŒ–ã€‚

        Args:
            n_trials: è¯•éªŒæ¬¡æ•°
            metric: ä¼˜åŒ–æŒ‡æ ‡
            direction: ä¼˜åŒ–æ–¹å‘
            model_train_func: æ¨¡å‹è®­ç»ƒå‡½æ•°ï¼Œç”¨äºåœ¨æœ€ä½³å‚æ•°ä¸Šé‡æ–°è®­ç»ƒæ¨¡å‹
        """
        if not OPTUNA_AVAILABLE:
            raise ImportError("Install optuna: pip install optuna")

        self.config = HPOConfig(n_trials=n_trials, metric=metric, direction=direction)
        self.study = None
        self.search_spaces = {}
        self.model_train_func = model_train_func
        self.best_model = None
        self.best_params = None
        self.best_score = None

        # åˆ›å»ºç»“æœç›®å½•
        Path(self.config.save_dir).mkdir(parents=True, exist_ok=True)
        logger.info(f"HPO initialized: {n_trials} trials")

    def add_param(self, name: str, param_type: str, low=None, high=None, choices=None) -> 'HyperparameterOptimizer':
        """
        é“¾å¼æ·»åŠ æœç´¢å‚æ•°ã€‚

        Args:
            name: å‚æ•°å
            param_type: å‚æ•°ç±»å‹ (float/int/categorical/log_float)
            low, high: æ•°å€¼èŒƒå›´
            choices: åˆ†ç±»é€‰é¡¹

        Returns:
            selfï¼Œæ”¯æŒé“¾å¼è°ƒç”¨
        """
        self.search_spaces[name] = {
            'type': param_type,
            'low': low,
            'high': high,
            'choices': choices
        }
        return self

    def optimize(self, eval_func: Callable[[Dict[str, Any], Any, Any], float], 
                 X_train: Any = None, y_train: Any = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œä¼˜åŒ–ã€‚

        Args:
            eval_func: è¯„ä¼°å‡½æ•°ï¼Œæ¥æ”¶(å‚æ•°å­—å…¸, X_train, y_train)ï¼Œè¿”å›CVåˆ†æ•°
            X_train: è®­ç»ƒç‰¹å¾æ•°æ®
            y_train: è®­ç»ƒç›®æ ‡æ•°æ®

        Returns:
            ä¼˜åŒ–ç»“æœå­—å…¸ï¼ŒåŒ…å«è®­ç»ƒå¥½çš„æœ€ä½³æ¨¡å‹
        """
        if not self.search_spaces:
            raise ValueError("No search spaces defined")

        # åˆ›å»ºç ”ç©¶
        self.study = optuna.create_study(
            direction=self.config.direction,
            sampler=optuna.samplers.TPESampler()
        )

        # ä¼˜åŒ– - ä¼ å…¥è®­ç»ƒæ•°æ®
        self.study.optimize(
            lambda trial: self._objective(trial, eval_func, X_train, y_train),
            n_trials=self.config.n_trials
        )

        # ä¿å­˜æœ€ä½³å‚æ•°å’Œåˆ†æ•°
        self.best_params = self.study.best_params
        self.best_score = self.study.best_value

        # ğŸ”§ å…³é”®ä¿®å¤ï¼šè®­ç»ƒå¹¶ä¿å­˜æœ€ä½³æ¨¡å‹
        logger.info(f"Training best model with params: {self.best_params}")
        if self.model_train_func and X_train is not None and y_train is not None:
            try:
                # ç”¨æœ€ä½³å‚æ•°åœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹
                self.best_model = self.model_train_func(self.best_params, X_train, y_train)
                logger.info("âœ… Best model trained and saved successfully")
            except Exception as e:
                logger.error(f"âŒ Failed to train best model: {e}")
                self.best_model = None
        else:
            logger.warning("âš ï¸ No model training function or training data provided, best_model will be None")

        # è¿”å›ç»“æœ
        result = {
            'best_params': self.best_params,
            'best_score': self.best_score,
            'best_model': self.best_model,  # ğŸ”§ æ·»åŠ æœ€ä½³æ¨¡å‹
            'n_trials': len(self.study.trials),
            'study_name': f"hpo_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}"
        }

        # ä¿å­˜ç»“æœ
        self._save_results(result)

        logger.info(f"Optimization done. Best {self.config.metric}: {result['best_score']:.4f}")
        if result['best_model'] is not None:
            logger.info("âœ… Optimization includes trained model")
        else:
            logger.warning("âš ï¸ Optimization result missing trained model")
        return result

    def _objective(self, trial: optuna.Trial, eval_func: Callable, X_train: Any, y_train: Any) -> float:
        """å•è¡Œç›®æ ‡å‡½æ•°å®ç°ã€‚"""
        # å»ºè®®å‚æ•°
        params = {}
        for name, space in self.search_spaces.items():
            if space['type'] == 'categorical':
                params[name] = trial.suggest_categorical(name, space['choices'])
            elif space['type'] == 'int':
                params[name] = trial.suggest_int(name, int(space['low']), int(space['high']))
            elif space['type'] == 'float':
                params[name] = trial.suggest_float(name, space['low'], space['high'])
            elif space['type'] == 'log_float':
                params[name] = trial.suggest_loguniform(name, space['low'], space['high'])

        # è¯„ä¼°å¹¶è¿”å› - ä¼ å…¥è®­ç»ƒæ•°æ®
        try:
            return eval_func(params, X_train, y_train)
        except Exception as e:
            logger.warning(f"Trial failed: {e}")
            return float('-inf') if self.config.direction == "maximize" else float('inf')

    def _save_results(self, result: Dict[str, Any]) -> None:
        """æç®€ç»“æœä¿å­˜ã€‚"""
        results_dir = Path(self.config.save_dir)

        # ä¿å­˜æœ€ä½³å‚æ•°
        with open(results_dir / f"{result['study_name']}_best.txt", 'w') as f:
            f.write(f"Best {self.config.metric}: {result['best_score']:.4f}\n")
            f.write("Best parameters:\n")
            for k, v in result['best_params'].items():
                f.write(f"  {k}: {v}\n")

        logger.info(f"Results saved to {results_dir}")


# ä¾¿æ·å‡½æ•° - ä¸€æ­¥åˆ›å»ºå¸¸ç”¨ä¼˜åŒ–å™¨
def create_xgboost_hpo(n_trials: int = 50) -> HyperparameterOptimizer:
    """
    åˆ›å»ºXGBoostè¶…å‚æ•°ä¼˜åŒ–å™¨ã€‚

    Args:
        n_trials: è¯•éªŒæ¬¡æ•°
    """
    def xgboost_train_func(best_params: Dict[str, Any], X_train: Any = None, y_train: Any = None) -> Any:
        """XGBoostæ¨¡å‹è®­ç»ƒå‡½æ•°ã€‚"""
        try:
            from ...models.implementations.xgboost_model import XGBoostModel

            if X_train is None or y_train is None:
                logger.warning("âš ï¸ No training data provided for XGBoost model training")
                return None

            # åˆ›å»ºå¹¶è®­ç»ƒæœ€ä½³æ¨¡å‹
            model = XGBoostModel(config=best_params)
            
            if hasattr(X_train, 'empty') and hasattr(y_train, 'empty'):
                # Pandas DataFrame/Series
                if X_train.empty or y_train.empty:
                    logger.error("âŒ Empty training data provided")
                    return None
            elif len(X_train) == 0 or len(y_train) == 0:
                # NumPy arrays or lists
                logger.error("âŒ Empty training data provided")
                return None

            model.fit(X_train, y_train)
            logger.info(f"âœ… XGBoost model trained with shape: {X_train.shape if hasattr(X_train, 'shape') else len(X_train)}")
            return model

        except Exception as e:
            logger.error(f"âŒ XGBoost model training failed: {e}")
            return None

    return (HyperparameterOptimizer(n_trials, model_train_func=xgboost_train_func)
            .add_param('n_estimators', 'int', 50, 500)
            .add_param('max_depth', 'int', 3, 12)
            .add_param('learning_rate', 'log_float', 0.01, 0.3)
            .add_param('subsample', 'float', 0.6, 1.0))


def create_metamodel_hpo(n_trials: int = 50) -> HyperparameterOptimizer:
    """åˆ›å»ºMetaModelè¶…å‚æ•°ä¼˜åŒ–å™¨ã€‚"""
    return (HyperparameterOptimizer(n_trials)
            .add_param('method', 'categorical', choices=['equal', 'lasso', 'ridge'])
            .add_param('alpha', 'log_float', 0.01, 10.0)
            .add_param('min_weight', 'float', 0.0, 0.1)
            .add_param('max_weight', 'float', 0.3, 1.0))


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æç®€ä½¿ç”¨æ–¹å¼
    def dummy_eval(params):
        return np.random.normal(0.5, 0.1)  # æ¨¡æ‹Ÿè¯„ä¼°

    # ä¸€è¡Œåˆ›å»ºä¼˜åŒ–å™¨
    optimizer = create_xgboost_hpo(20)

    # ä¸€è¡Œæ‰§è¡Œä¼˜åŒ–
    result = optimizer.optimize(dummy_eval)

    print(f"Best score: {result['best_score']:.4f}")
    print(f"Best params: {result['best_params']}")