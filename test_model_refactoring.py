#!/usr/bin/env python3
"""
Integration Test for Model Architecture Refactoring

Tests the complete refactoring including:
1. MomentumRankingModel implementation
2. ModelPredictor three initialization modes
3. StrategyFactory model loading
4. End-to-end strategy execution

Date: 2025-10-02
"""

import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path

# Add src to path
sys.path.insert(0, '/Users/wenjiaqi/Downloads/bloomberg-competition/src')

def create_test_momentum_data():
    """Create test data with clear momentum patterns."""
    symbols = ['HIGH_MOM', 'MED_MOM', 'LOW_MOM', 'NEG_MOM', 'FLAT']
    dates = pd.date_range(start='2023-01-01', periods=300, freq='D')
    
    price_data = {}
    
    # Create assets with different momentum characteristics
    momentum_patterns = {
        'HIGH_MOM': 0.003,   # Strong positive momentum
        'MED_MOM': 0.002,    # Medium positive momentum
        'LOW_MOM': 0.001,    # Low positive momentum
        'NEG_MOM': -0.001,   # Negative momentum
        'FLAT': 0.0          # No momentum
    }
    
    for symbol in symbols:
        base_price = 100
        drift = momentum_patterns[symbol]
        returns = np.random.normal(drift, 0.01, len(dates))
        
        prices = [base_price]
        for ret in returns:
            new_price = prices[-1] * (1 + ret)
            prices.append(max(new_price, 1))
        prices = prices[1:]
        
        df = pd.DataFrame({
            'Open': prices * np.random.uniform(0.99, 1.01, len(prices)),
            'High': prices * np.random.uniform(1.00, 1.02, len(prices)),
            'Low': prices * np.random.uniform(0.98, 1.00, len(prices)),
            'Close': prices,
            'Volume': np.random.randint(1000000, 5000000, len(prices))
        }, index=dates)
        
        df['High'] = df[['High', 'Open', 'Close']].max(axis=1)
        df['Low'] = df[['Low', 'Open', 'Close']].min(axis=1)
        
        price_data[symbol] = df
    
    return price_data

def create_test_features():
    """Create test momentum features."""
    data = {
        'momentum_21d': [0.05, 0.10, 0.02, -0.01, 0.08],
        'momentum_63d': [0.08, 0.12, 0.03, 0.01, 0.09],
        'momentum_252d': [0.15, 0.20, 0.05, 0.02, 0.18]
    }
    
    X = pd.DataFrame(data, index=['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'AMZN'])
    y = pd.Series([0.02, 0.03, 0.01, -0.01, 0.025], index=X.index)
    
    return X, y

# ============================================================================
# Test 1: MomentumRankingModel - Rule-based Mode
# ============================================================================

def test_momentum_model_rule_based():
    """Test MomentumRankingModel in rule-based mode."""
    print("\n" + "=" * 70)
    print("TEST 1: MomentumRankingModel - Rule-based Mode")
    print("=" * 70)
    
    try:
        from trading_system.models import MomentumRankingModel, ModelStatus
        
        # Create rule-based model
        config = {
            'mode': 'rule_based',
            'top_n': 3,
            'min_momentum': 0.0,
            'momentum_weights': [0.3, 0.3, 0.4],
            'momentum_periods': [21, 63, 252]
        }
        
        model = MomentumRankingModel(config=config)
        print(f"‚úì Created rule-based MomentumRankingModel")
        print(f"  - Mode: {model.mode}")
        print(f"  - Top N: {model.top_n}")
        print(f"  - Weights: {model.momentum_weights}")
        
        # Create test data
        X, y = create_test_features()
        print(f"\n‚úì Created test data:")
        print(f"  - Samples: {len(X)}")
        print(f"  - Features: {list(X.columns)}")
        
        # Train (just sets status for rule-based)
        model.fit(X, y)
        assert model.status == ModelStatus.TRAINED, "Model should be trained"
        print(f"\n‚úì Model trained (status: {model.status})")
        
        # Predict momentum scores
        scores = model.predict(X)
        print(f"\n‚úì Generated momentum scores:")
        for symbol, score in zip(X.index, scores):
            print(f"  - {symbol}: {score:.4f}")
        
        # Verify scores are reasonable
        assert len(scores) == len(X), "Should have one score per asset"
        assert scores[1] > scores[2], "MSFT should have higher momentum than GOOGL"
        print(f"\n‚úì Scores validation passed")
        
        # Get top N signals
        signals = model.get_top_n_signals(X)
        print(f"\n‚úì Generated top {model.top_n} signals:")
        for symbol, signal in signals.items():
            if signal > 0:
                print(f"  - {symbol}: {signal} (SELECTED)")
        
        selected_count = (signals > 0).sum()
        assert selected_count == min(model.top_n, len(X)), f"Should select {model.top_n} assets"
        print(f"\n‚úì Selection validation passed: {selected_count} assets selected")
        
        # Test feature importance
        importance = model.get_feature_importance()
        print(f"\n‚úì Feature importance:")
        for feature, weight in importance.items():
            print(f"  - {feature}: {weight:.4f}")
        
        print(f"\n‚úÖ Rule-based MomentumRankingModel test PASSED")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Test FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False

# ============================================================================
# Test 2: MomentumRankingModel - Trainable Mode
# ============================================================================

def test_momentum_model_trainable():
    """Test MomentumRankingModel in trainable mode."""
    print("\n" + "=" * 70)
    print("TEST 2: MomentumRankingModel - Trainable Mode")
    print("=" * 70)
    
    try:
        from trading_system.models import MomentumRankingModel, ModelStatus
        
        # Create trainable model
        config = {
            'mode': 'trainable',
            'top_n': 3,
            'min_momentum': 0.0,
            'momentum_periods': [21, 63, 252]
        }
        
        model = MomentumRankingModel(config=config)
        print(f"‚úì Created trainable MomentumRankingModel")
        print(f"  - Mode: {model.mode}")
        
        # Create test data
        X, y = create_test_features()
        
        # Train (learns weights)
        initial_weights = model.momentum_weights.copy()
        print(f"\n  Initial weights: {initial_weights}")
        
        model.fit(X, y)
        learned_weights = model.momentum_weights
        print(f"  Learned weights: {learned_weights}")
        
        # Verify weights were learned
        assert model.status == ModelStatus.TRAINED
        print(f"\n‚úì Model learned weights (status: {model.status})")
        
        # Verify weights are valid
        assert len(learned_weights) == 3
        assert all(w >= 0 for w in learned_weights), "All weights should be non-negative"
        assert abs(learned_weights.sum() - 1.0) < 1e-6, "Weights should sum to 1"
        print(f"‚úì Learned weights validation passed")
        
        # Test prediction
        scores = model.predict(X)
        assert len(scores) == len(X)
        print(f"\n‚úì Prediction works with learned weights")
        
        print(f"\n‚úÖ Trainable MomentumRankingModel test PASSED")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Test FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False

# ============================================================================
# Test 3: ModelPredictor - Three Initialization Modes
# ============================================================================

def test_model_predictor_initialization():
    """Test ModelPredictor's three initialization modes."""
    print("\n" + "=" * 70)
    print("TEST 3: ModelPredictor - Three Initialization Modes")
    print("=" * 70)
    
    try:
        from trading_system.models.serving.predictor import ModelPredictor
        from trading_system.models import ModelFactory, MomentumRankingModel
        
        # Mode 1: Create from model_id
        print("\n--- Mode 1: Initialize with model_id ---")
        predictor1 = ModelPredictor(
            model_id='momentum_ranking',
            enable_monitoring=False
        )
        assert predictor1._current_model is not None, "Model should be loaded"
        assert predictor1._current_model_id is not None, "Model ID should be set"
        print(f"‚úì Created predictor from model_id: '{predictor1._current_model_id}'")
        print(f"  - Model type: {predictor1._current_model.model_type}")
        print(f"  - Model status: {predictor1._current_model.status}")
        
        # Mode 2: Create from model_instance
        print("\n--- Mode 2: Initialize with model_instance ---")
        custom_model = ModelFactory.create('momentum_ranking', config={
            'mode': 'rule_based',
            'top_n': 10,
            'momentum_weights': [0.2, 0.3, 0.5]
        })
        
        predictor2 = ModelPredictor(
            model_instance=custom_model,
            enable_monitoring=False
        )
        assert predictor2._current_model is custom_model, "Should use injected model"
        assert predictor2._current_model.config['top_n'] == 10, "Should preserve config"
        print(f"‚úì Created predictor from model_instance")
        print(f"  - Model type: {predictor2._current_model.model_type}")
        print(f"  - Custom top_n: {predictor2._current_model.top_n}")
        
        # Mode 3: Create empty then load (backward compatibility)
        print("\n--- Mode 3: Initialize empty then load ---")
        predictor3 = ModelPredictor(enable_monitoring=False)
        assert predictor3._current_model is None, "Model should be None initially"
        print(f"‚úì Created empty predictor")
        
        # Load model using load_model method
        model_id = predictor3.load_model('momentum_ranking')
        assert predictor3._current_model is not None, "Model should be loaded"
        print(f"‚úì Loaded model via load_model(): {model_id}")
        
        print(f"\n‚úÖ ModelPredictor initialization test PASSED")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Test FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False

# ============================================================================
# Test 4: ModelPredictor - Prediction with MomentumModel
# ============================================================================

def test_model_predictor_prediction():
    """Test ModelPredictor prediction with MomentumRankingModel."""
    print("\n" + "=" * 70)
    print("TEST 4: ModelPredictor - Prediction Workflow")
    print("=" * 70)
    
    try:
        from trading_system.models.serving.predictor import ModelPredictor
        from trading_system.models import MomentumRankingModel
        
        # Create and train model
        model = MomentumRankingModel(config={
            'mode': 'rule_based',
            'top_n': 3
        })
        
        X_train, y_train = create_test_features()
        model.fit(X_train, y_train)
        print(f"‚úì Trained MomentumRankingModel")
        
        # Create predictor with trained model
        predictor = ModelPredictor(
            model_instance=model,
            enable_monitoring=False
        )
        print(f"‚úì Created ModelPredictor with trained model")
        
        # Create test features for prediction
        test_features = pd.DataFrame({
            'momentum_21d': [0.06],
            'momentum_63d': [0.09],
            'momentum_252d': [0.16]
        })
        
        # Make prediction
        result = predictor.predict(
            features=test_features,
            symbol='TEST',
            prediction_date=datetime(2023, 10, 1)
        )
        
        print(f"\n‚úì Prediction result:")
        print(f"  - Symbol: {result['symbol']}")
        print(f"  - Prediction: {result['prediction']:.6f}")
        print(f"  - Model ID: {result['model_id']}")
        print(f"  - Timestamp: {result['timestamp']}")
        
        assert 'prediction' in result, "Should have prediction value"
        assert result['symbol'] == 'TEST', "Should match input symbol"
        assert isinstance(result['prediction'], float), "Prediction should be float"
        
        print(f"\n‚úÖ ModelPredictor prediction test PASSED")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Test FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False

# ============================================================================
# Test 5: StrategyFactory - Model Loading Modes
# ============================================================================

def test_strategy_factory_model_loading():
    """Test StrategyFactory's three model loading modes."""
    print("\n" + "=" * 70)
    print("TEST 5: StrategyFactory - Model Loading")
    print("=" * 70)
    
    try:
        from trading_system.strategies.factory import StrategyFactory
        
        print(f"‚úì Imported StrategyFactory")
        
        # Test Mode 3: Create new model instance from factory
        print("\n--- Testing Mode 3: Create from Factory ---")
        
        config = {
            'type': 'dual_momentum',  # This would use momentum_ranking model
            'name': 'test_dual_momentum',
            'model_id': 'momentum_ranking',
            'model_config': {
                'mode': 'rule_based',
                'top_n': 5,
                'min_momentum': 0.0,
                'momentum_weights': [0.3, 0.3, 0.4]
            },
            'enable_monitoring': False
        }
        
        # Note: This will fail if dual_momentum strategy isn't updated to use the new architecture
        # For now, we'll just test the factory's _create_model_predictor method
        
        from trading_system.models.serving.predictor import ModelPredictor
        predictor = StrategyFactory._create_model_predictor(
            model_id='momentum_ranking',
            config=config
        )
        
        assert predictor is not None, "Predictor should be created"
        assert predictor._current_model is not None, "Model should be loaded"
        assert predictor._current_model.model_type == 'momentum_ranking'
        
        print(f"‚úì Created ModelPredictor via StrategyFactory")
        print(f"  - Model type: {predictor._current_model.model_type}")
        print(f"  - Model status: {predictor._current_model.status}")
        print(f"  - Top N: {predictor._current_model.top_n}")
        
        # Test model type inference
        print("\n--- Testing Model Type Inference ---")
        
        inferred_type = StrategyFactory._infer_model_type('momentum_ranking_v1')
        assert inferred_type == 'momentum_ranking', f"Should infer 'momentum_ranking' from 'momentum_ranking_v1'"
        print(f"‚úì Inferred 'momentum_ranking_v1' ‚Üí '{inferred_type}'")
        
        inferred_type2 = StrategyFactory._infer_model_type('ff5_regression')
        assert inferred_type2 == 'ff5_regression', "Should return as-is if no version"
        print(f"‚úì Inferred 'ff5_regression' ‚Üí '{inferred_type2}'")
        
        print(f"\n‚úÖ StrategyFactory model loading test PASSED")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Test FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False

# ============================================================================
# Test 6: ModelFactory Registration
# ============================================================================

def test_model_factory_registration():
    """Test that MomentumRankingModel is properly registered."""
    print("\n" + "=" * 70)
    print("TEST 6: ModelFactory - Model Registration")
    print("=" * 70)
    
    try:
        from trading_system.models import ModelFactory
        
        # Check available models
        available_models = ModelFactory.list_models()
        print(f"\n‚úì Available models in registry:")
        for model_type, info in available_models.items():
            print(f"  - {model_type}: {info['description']}")
        
        # Verify momentum_ranking is registered
        assert 'momentum_ranking' in available_models, "momentum_ranking should be registered"
        assert 'ff5_regression' in available_models, "ff5_regression should still be registered"
        
        print(f"\n‚úì MomentumRankingModel is registered")
        
        # Test creating model from registry
        mom_model = ModelFactory.create('momentum_ranking')
        assert mom_model is not None
        assert mom_model.model_type == 'momentum_ranking'
        print(f"‚úì Successfully created model from registry")
        
        # Test with custom config
        custom_model = ModelFactory.create('momentum_ranking', config={
            'mode': 'trainable',
            'top_n': 10
        })
        assert custom_model.mode == 'trainable'
        assert custom_model.top_n == 10
        print(f"‚úì Created model with custom config")
        
        # Test convenience function
        from trading_system.models.registry import create_momentum_model
        conv_model = create_momentum_model(config={'top_n': 7})
        assert conv_model.top_n == 7
        print(f"‚úì Convenience function works")
        
        print(f"\n‚úÖ ModelFactory registration test PASSED")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Test FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False

# ============================================================================
# Test 7: Model Serialization (Save/Load)
# ============================================================================

def test_model_serialization():
    """Test MomentumRankingModel save and load."""
    print("\n" + "=" * 70)
    print("TEST 7: MomentumRankingModel - Serialization")
    print("=" * 70)
    
    try:
        from trading_system.models import MomentumRankingModel
        import tempfile
        import shutil
        
        # Create and train model
        config = {
            'mode': 'trainable',
            'top_n': 5,
            'min_momentum': 0.01,
            'momentum_weights': [0.25, 0.35, 0.40]
        }
        
        original_model = MomentumRankingModel(config=config)
        X, y = create_test_features()
        original_model.fit(X, y)
        
        original_weights = original_model.momentum_weights.copy()
        original_scores = original_model.predict(X)
        
        print(f"‚úì Created and trained original model")
        print(f"  - Weights: {original_weights}")
        
        # Save model to temporary directory
        temp_dir = tempfile.mkdtemp()
        model_path = Path(temp_dir) / "test_momentum_model"
        
        try:
            original_model.save(model_path)
            print(f"\n‚úì Saved model to: {model_path}")
            
            # Verify files exist
            assert (model_path / "model.pkl").exists(), "model.pkl should exist"
            assert (model_path / "metadata.json").exists(), "metadata.json should exist"
            assert (model_path / "config.json").exists(), "config.json should exist"
            print(f"‚úì All model files created")
            
            # Load model
            loaded_model = MomentumRankingModel.load(model_path)
            print(f"\n‚úì Loaded model from disk")
            
            # Verify loaded model matches original
            assert loaded_model.mode == original_model.mode
            assert loaded_model.top_n == original_model.top_n
            assert np.allclose(loaded_model.momentum_weights, original_weights)
            print(f"‚úì Loaded model config matches original")
            
            # Verify predictions match
            loaded_scores = loaded_model.predict(X)
            assert np.allclose(loaded_scores, original_scores)
            print(f"‚úì Loaded model predictions match original")
            
            print(f"\n‚úÖ Model serialization test PASSED")
            return True
            
        finally:
            # Cleanup
            shutil.rmtree(temp_dir)
            print(f"\n‚úì Cleaned up temporary files")
        
    except Exception as e:
        print(f"\n‚ùå Test FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False

# ============================================================================
# Test 8: End-to-End Integration
# ============================================================================

def test_end_to_end_integration():
    """Test complete end-to-end workflow with real data."""
    print("\n" + "=" * 70)
    print("TEST 8: End-to-End Integration Test")
    print("=" * 70)
    
    try:
        from trading_system.models import MomentumRankingModel
        from trading_system.models.serving.predictor import ModelPredictor
        from trading_system.feature_engineering.pipeline import FeatureEngineeringPipeline
        from trading_system.feature_engineering.models.data_types import FeatureConfig
        
        # Step 1: Create price data with momentum
        print("\n--- Step 1: Prepare Data ---")
        price_data = create_test_momentum_data()
        print(f"‚úì Created test price data for {len(price_data)} symbols")
        
        # Step 2: Create feature pipeline
        print("\n--- Step 2: Feature Engineering ---")
        feature_config = FeatureConfig(
            enabled_features=['momentum'],
            momentum_periods=[21, 63, 252]
        )
        feature_pipeline = FeatureEngineeringPipeline(feature_config)
        
        # Compute features - wrap in expected format
        features = feature_pipeline.transform({'price_data': price_data})
        
        print(f"‚úì Computed features:")
        print(f"  - Shape: {features.shape}")
        print(f"  - Columns: {list(features.columns)[:10]}...")  # Show first 10
        
        # Filter to momentum features we need
        momentum_cols = [col for col in features.columns 
                        if any(f'momentum_{p}d' in col for p in [21, 63, 252])]
        
        if momentum_cols:
            print(f"  - Momentum features: {momentum_cols}")
        
        # Step 3: Create and train model
        print("\n--- Step 3: Train Model ---")
        model = MomentumRankingModel(config={
            'mode': 'rule_based',
            'top_n': 3,
            'min_momentum': 0.0
        })
        
        # For rule-based model, we just need to set status
        # Create dummy training data
        X_dummy, y_dummy = create_test_features()
        model.fit(X_dummy, y_dummy)
        
        print(f"‚úì Model trained (rule-based mode)")
        print(f"  - Status: {model.status}")
        print(f"  - Weights: {model.momentum_weights}")
        
        # Step 4: Create predictor
        print("\n--- Step 4: Create Predictor ---")
        predictor = ModelPredictor(
            model_instance=model,
            enable_monitoring=False
        )
        print(f"‚úì Created ModelPredictor")
        
        # Step 5: Make predictions (simulate)
        print("\n--- Step 5: Make Predictions ---")
        
        # Since we have complex features, let's just verify the model works
        test_features = pd.DataFrame({
            'momentum_21d': [0.05, 0.03, 0.08, -0.01, 0.00],
            'momentum_63d': [0.07, 0.04, 0.09, 0.01, 0.00],
            'momentum_252d': [0.12, 0.06, 0.15, 0.02, 0.01]
        }, index=['HIGH_MOM', 'MED_MOM', 'LOW_MOM', 'NEG_MOM', 'FLAT'])
        
        scores = model.predict(test_features)
        signals = model.get_top_n_signals(test_features)
        
        print(f"‚úì Generated predictions:")
        for symbol, score, signal in zip(test_features.index, scores, signals):
            status = "‚úì SELECTED" if signal > 0 else ""
            print(f"  - {symbol:10s}: score={score:.4f} {status}")
        
        # Verify top performers were selected
        selected_symbols = signals[signals > 0].index.tolist()
        assert len(selected_symbols) == 3, "Should select 3 assets"
        assert 'HIGH_MOM' in selected_symbols, "HIGH_MOM should be selected"
        assert 'LOW_MOM' in selected_symbols or 'MED_MOM' in selected_symbols
        
        print(f"\n‚úì Validation passed:")
        print(f"  - Selected {len(selected_symbols)} assets: {selected_symbols}")
        
        print(f"\n‚úÖ End-to-End integration test PASSED")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Test FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False

# ============================================================================
# Main Test Runner
# ============================================================================

def main():
    """Run all refactoring tests."""
    print("\n" + "=" * 70)
    print("MODEL ARCHITECTURE REFACTORING - INTEGRATION TESTS")
    print("=" * 70)
    print(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    tests = [
        ("MomentumModel - Rule-based", test_momentum_model_rule_based),
        ("MomentumModel - Trainable", test_momentum_model_trainable),
        ("ModelPredictor - Initialization", test_model_predictor_initialization),
        ("ModelPredictor - Prediction", test_model_predictor_prediction),
        ("StrategyFactory - Loading", test_strategy_factory_model_loading),
        ("ModelFactory - Registration", test_model_factory_registration),
        ("Model Serialization", test_model_serialization),
        ("End-to-End Integration", test_end_to_end_integration),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        result = test_func()
        results.append((test_name, result))
    
    # Print summary
    print("\n" + "=" * 70)
    print("TEST SUMMARY")
    print("=" * 70)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status} - {test_name}")
    
    print("\n" + "=" * 70)
    print(f"Results: {passed}/{total} tests passed ({passed/total*100:.1f}%)")
    print("=" * 70)
    
    if passed == total:
        print("\nüéâ ALL TESTS PASSED! Model refactoring is working correctly.")
        return 0
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} test(s) failed. Please check the output above.")
        return 1

if __name__ == "__main__":
    exit(main())

