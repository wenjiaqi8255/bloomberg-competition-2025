"""
Fama-French 5-Factor Strategy - Unified Architecture

This strategy implements the academic Fama-French 5-factor model
using the unified pipeline â†’ model â†’ position sizing architecture.

Fama-French 5 Factors:
1. MKT (Market): Market risk premium
2. SMB (Size): Small Minus Big market cap
3. HML (Value): High Minus Low book-to-market
4. RMW (Profitability): Robust Minus Weak profitability
5. CMA (Investment): Conservative Minus Aggressive investment

Architecture:
    FactorDataProvider â†’ FF5RegressionModel 

Key Difference from other strategies:
    - FF5 uses FACTOR DATA directly from FF5DataProvider
    - Does NOT use technical indicators or traditional feature engineering
    - Model expects columns: ['MKT', 'SMB', 'HML', 'RMW', 'CMA']

Data Flow:
    1. FF5DataProvider provides factor data (MKT, SMB, HML, RMW, CMA, RF)
    2. For each stock, we create factor features based on historical returns
    3. FF5RegressionModel estimates factor betas and predicts expected returns

Model:
    - FF5RegressionModel (ALREADY IMPLEMENTED!)
    - âœ… models/implementations/ff5_model.py
    - Estimates factor betas via linear regression
    - Predicts expected returns from factor exposures

Position Sizing:
    - Volatility-based position sizing
    - Maximum position weight constraints

Key Insight:
    This is a LINEAR MODEL that can be TRAINED:
    - Estimates factor loadings (betas) from historical data
    - Predicts expected returns: E[R] = Î²_MKT*MKT + Î²_SMB*SMB + ...
    - Can be backtested with proper train/test methodology
"""

import logging
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional, List, Any
import pandas as pd
import numpy as np

from .base_strategy import BaseStrategy
from ..feature_engineering.pipeline import FeatureEngineeringPipeline
from ..models.serving.predictor import ModelPredictor

logger = logging.getLogger(__name__)


class FamaFrench5Strategy(BaseStrategy):
    """
    Fama-French 5-factor strategy using the unified architecture.
    
    This strategy demonstrates how academic factor models fit into
    the unified framework - they're just linear regression models!
    
    Good News: The model is ALREADY IMPLEMENTED!
    âœ… models/implementations/ff5_model.py exists and works
    
    Example Usage:
        # Create feature pipeline for FF5 factors
        from feature_engineering.models.data_types import FeatureConfig
        
        # Configure pipeline to compute the 5 factors
        # (When fundamental data unavailable, use technical proxies)
        feature_config = FeatureConfig(
            enabled_features=['momentum', 'volatility', 'volume'],
            # These will be used to construct factor proxies:
            # - MKT: market returns
            # - SMB: volatility (small cap proxy)
            # - HML: momentum reversal (value proxy)
            # - RMW: return consistency (profitability proxy)
            # - CMA: low volatility (conservative proxy)
        )
        feature_pipeline = FeatureEngineeringPipeline(feature_config)
        
        # Fit pipeline on training data
        feature_pipeline.fit(train_data)
        
        # Load FF5 model (ALREADY EXISTS!)
        model_predictor = ModelPredictor(model_id="ff5_regression_v1")
        
        # The FF5RegressionModel will:
        # 1. During training: estimate factor betas for each stock
        # 2. During prediction: calculate expected returns from factors
        
        # Create strategy
        strategy = FamaFrench5Strategy(
            name="FF5",
            feature_pipeline=feature_pipeline,
            model_predictor=model_predictor,
            lookback_days=252,
            risk_free_rate=0.02
        )
        
        # Generate signals
        signals = strategy.generate_signals(price_data, start_date, end_date)
    """
    
    def __init__(self,
                 name: str,
                 feature_pipeline: FeatureEngineeringPipeline,
                 model_predictor: ModelPredictor,
                 lookback_days: int = 252,
                 risk_free_rate: float = 0.02,
                 **kwargs):
        """
        Initialize Fama-French 5-factor strategy.

        Args:
            name: Strategy identifier
            feature_pipeline: Fitted pipeline (computes technical indicators and merges factor data)
            model_predictor: Predictor with FF5RegressionModel loaded
            lookback_days: Lookback period for factor calculation
            risk_free_rate: Risk-free rate for excess return calculation
            **kwargs: Additional parameters
        """
        super().__init__(
            name=name,
            feature_pipeline=feature_pipeline,
            model_predictor=model_predictor,
            **kwargs
        )

        self.lookback_days = lookback_days
        self.risk_free_rate = risk_free_rate

        logger.info(f"Initialized FamaFrench5Strategy '{name}' with "
                   f"lookback={lookback_days}d, rf_rate={risk_free_rate}")
        logger.info(f"[{name}] Following SOLID principles - strategy as pure delegate")

    def _compute_features(self, pipeline_data: Dict[str, Any]) -> pd.DataFrame:
        """
        Compute features for FF5 strategy following the new architecture.

        âœ… REFACTORED: Following "Data preparation responsibility moves up to orchestrator" pattern.
        FF5 strategy only uses pipeline_data provided by orchestrator, doesn't access providers.

        Args:
            pipeline_data: Complete data prepared by orchestrator
                - 'price_data': Dict[str, DataFrame] - OHLCV price data
                - 'factor_data': DataFrame - FF5 factor data (MKT, SMB, HML, RMW, CMA, RF)

        Returns:
            DataFrame with features computed by the FeatureEngineeringPipeline
        """
        logger.info(f"[{self.name}] ðŸ”„ Computing features from pipeline data (new architecture)")
        logger.info(f"[{self.name}] Pipeline data keys: {list(pipeline_data.keys())}")

        # Extract price_data and factor_data from pipeline_data
        price_data = pipeline_data.get('price_data', {})
        factor_data = pipeline_data.get('factor_data')

        # Validate required data
        if not price_data:
            logger.error(f"[{self.name}] âŒ No price_data in pipeline_data")
            return pd.DataFrame()

        if factor_data is None:
            logger.error(f"[{self.name}] âŒ No factor_data in pipeline_data - FF5 strategy requires factor data!")
            return pd.DataFrame()

        # Log data info
        symbols_count = len(price_data)
        factor_shape = factor_data.shape if hasattr(factor_data, 'shape') else 'N/A'
        logger.info(f"[{self.name}] Processing {symbols_count} symbols with factor data shape: {factor_shape}")

        # Prepare pipeline input
        pipeline_input = {
            'price_data': price_data,
            'factor_data': factor_data
        }

        try:
            # Delegate to the existing feature pipeline (DRY principle)
            # The pipeline handles technical indicators, factor data merging, and time alignment
            features = self.feature_pipeline.transform(pipeline_input)

            logger.info(f"[{self.name}] âœ… Pipeline computed features: {features.shape}")
            logger.debug(f"[{self.name}] Feature columns: {list(features.columns)}")

            # CRITICAL: Check if FF5 factors are present
            expected_factors = self._expected_factor_columns()
            available_factors = [col for col in features.columns if col in expected_factors]
            missing_factors = set(expected_factors) - set(available_factors)

            if missing_factors:
                logger.error(f"[{self.name}] âŒ Missing FF5 factors: {missing_factors}")
                logger.error(f"[{self.name}] Available columns: {list(features.columns[:10])}...")
            else:
                logger.info(f"[{self.name}] âœ… All FF5 factors present: {available_factors}")

            return features

        except Exception as e:
            logger.error(f"[{self.name}] Error computing features via pipeline: {e}")
            logger.debug(f"[{self.name}] Error details:", exc_info=True)
            return pd.DataFrame()

    def _expected_factor_columns(self) -> list:
        """Get the expected factor columns for FF5 model."""
        return ['MKT', 'SMB', 'HML', 'RMW', 'CMA']

    def generate_signals(self, pipeline_data, start_date, end_date):
        """Generate signals with pipeline data context"""
        
        # âœ… ä¿å­˜pipeline_dataä»¥ä¾¿_get_predictionsè®¿é—®
        self._current_pipeline_data = pipeline_data
        
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        return super().generate_signals(pipeline_data, start_date, end_date)

    def _get_predictions(self, features, price_data, start_date, end_date):
        # """
        # FF5é¢„æµ‹ä½¿ç”¨pipeline_dataä¸­çš„factor_data
        # """
        # if not hasattr(self, '_current_pipeline_data'):
        #     return super()._get_predictions(features, price_data, start_date, end_date)
        
        # factor_data = self._current_pipeline_data.get('factor_data')
        # if factor_data is None:
        #     logger.error("No factor_data in pipeline_data")
        #     return pd.DataFrame()
        
        # current_model = self.model_predictor.get_current_model()
        # symbols = list(price_data.keys())
        
        # predictions_list = []
        
        # # å¯¹é¢„æµ‹æ—¥æœŸèŒƒå›´å†…çš„æ¯ä¸€å¤©
        # for date in pd.date_range(start_date, end_date):
        #     if date not in factor_data.index:
        #         logger.warning(f"Date {date} not in factor_data")
        #         continue
            
        #     # èŽ·å–è¯¥æ—¥æœŸçš„å› å­å€¼
        #     date_factors = factor_data.loc[[date], ['MKT', 'SMB', 'HML', 'RMW', 'CMA']]
            
        #     # é¢„æµ‹
        #     preds = current_model.predict(date_factors, symbols=symbols)
            
        #     if isinstance(preds, np.ndarray):
        #         preds = pd.Series(preds, index=symbols, name=date)
            
        #     predictions_list.append(preds)
        
        # if predictions_list:
        #     return pd.concat(predictions_list, axis=1).T
        # else:
        #     return pd.DataFrame()
        """
        FF5é¢„æµ‹çŽ°åœ¨ç›´æŽ¥ä½¿ç”¨æ¨¡åž‹çš„alphaå€¼ä½œä¸ºä¿¡å·ã€‚
        è¿™æ›´ç¬¦åˆå› å­æŠ•èµ„çš„åˆè¡·ï¼šå¯»æ‰¾æŒç»­äº§ç”Ÿè¶…é¢å›žæŠ¥çš„èµ„äº§ã€‚
        """
        logger.info("ä¿®æ­£ï¼šFF5ç­–ç•¥çŽ°åœ¨ä½¿ç”¨æ¨¡åž‹çš„ Alpha ä½œä¸ºå›ºå®šä¿¡å·ã€‚")
        
        current_model = self.model_predictor.get_current_model()
        if not hasattr(current_model, 'get_symbol_alphas'):
            logger.error("å½“å‰æ¨¡åž‹ä¸æ”¯æŒ get_symbol_alphas æ–¹æ³•ï¼Œæ— æ³•èŽ·å– Alphaã€‚")
            return pd.DataFrame()

        # ä¸€æ¬¡æ€§èŽ·å–æ‰€æœ‰å·²è®­ç»ƒè‚¡ç¥¨çš„ alpha å€¼
        alphas = current_model.get_symbol_alphas()
        if not alphas:
            logger.error("æœªèƒ½ä»Žæ¨¡åž‹ä¸­èŽ·å–ä»»ä½• Alpha å€¼ã€‚")
            return pd.DataFrame()

        # Apply alpha significance filter if enabled
        alpha_config = self.parameters.get('alpha_significance', {})
        if alpha_config.get('enabled', False):
            alphas = self._apply_alpha_significance_filter(alphas, alpha_config)

        # å°† alpha è½¬æ¢ä¸ºä¸€ä¸ª Seriesï¼Œè¿™æ˜¯æˆ‘ä»¬çš„ä¿¡å·
        alpha_signals = pd.Series(alphas)

        # åˆ›å»ºä¸€ä¸ªç¬¦åˆå›žæµ‹æ—¶é—´èŒƒå›´çš„ DataFrame
        # æ¯ä¸€å¤©çš„ä¿¡å·éƒ½æ˜¯å›ºå®šä¸å˜çš„ alpha å€¼
        date_range = pd.date_range(start_date, end_date, freq='D')
        predictions_df = pd.DataFrame(index=date_range)

        for symbol, alpha_value in alpha_signals.items():
            predictions_df[symbol] = alpha_value
            
        # ç¡®ä¿è¿”å›žçš„ DataFrame åŒ…å«æ­£ç¡®çš„è‚¡ç¥¨
        symbols_in_data = list(price_data.keys())
        predictions_df = predictions_df.reindex(columns=symbols_in_data).fillna(0.0)

        logger.info(f"æˆåŠŸä¸º {len(alpha_signals)} åªè‚¡ç¥¨ç”Ÿæˆäº†åŸºäºŽ Alpha çš„ä¿¡å·ã€‚")
        return predictions_df

    def _apply_alpha_significance_filter(self, alphas: Dict[str, float], config: Dict[str, Any]) -> Dict[str, float]:
        """
        Apply significance filter to alphas based on t-statistics.
        
        Filters out or shrinks statistically insignificant alphas to prevent
        MVO from over-weighting stocks with noisy alpha estimates.
        
        Args:
            alphas: Dictionary of {symbol: alpha_value}
            config: Configuration dict with keys:
                - enabled: bool (checked by caller)
                - t_threshold: float (default 2.0)
                - method: str ('hard_threshold', 'linear_shrinkage', 'sigmoid_shrinkage')
                - tstats_path: str (path to CSV file)
        
        Returns:
            Filtered alphas dict (modified in-place for performance, but returns for clarity)
        """
        path = config.get('tstats_path', './alpha_tstats.csv')
        # Resolve path: if relative, make it relative to project root (where run_experiment is executed)
        if not os.path.isabs(path):
            # Try current directory first (for compatibility)
            if os.path.exists(path):
                pass  # Use as-is
            else:
                # Try project root (common case)
                project_root = Path(__file__).parent.parent.parent.parent
                root_path = project_root / path
                if root_path.exists():
                    path = str(root_path)
                else:
                    logger.warning(f"T-stat file not found at {path} or {root_path}, will try to load anyway")
        
        threshold = float(config.get('t_threshold', 2.0))
        method = config.get('method', 'hard_threshold')
        
        # Store before state for logging
        alphas_before = alphas.copy()
        mean_before = np.mean(list(alphas_before.values())) if alphas_before else 0.0
        std_before = np.std(list(alphas_before.values())) if alphas_before else 0.0
        nz_before = sum(1 for v in alphas_before.values() if v != 0.0)
        
        try:
            tstat_df = pd.read_csv(path)
            
            # Validate CSV format
            if 'symbol' not in tstat_df.columns or 't_alpha' not in tstat_df.columns:
                logger.warning(
                    f"Invalid t-stat CSV format: missing required columns. "
                    f"Expected: symbol, t_alpha. Found: {list(tstat_df.columns)}"
                )
                return alphas
            
            # Build lookup dict for O(1) access
            tstat_dict = tstat_df.set_index('symbol')['t_alpha'].to_dict()
            
            n_total = len(alphas)
            n_filtered = 0
            n_missing = 0
            
            for symbol in list(alphas.keys()):
                if symbol not in tstat_dict:
                    n_missing += 1
                    logger.debug(f"Symbol {symbol} not in t-stat CSV, keeping original alpha")
                    continue
                
                t_stat = tstat_dict[symbol]
                
                # Handle NaN
                if pd.isna(t_stat):
                    logger.debug(f"Symbol {symbol} has NaN t-stat, setting alpha=0")
                    alphas[symbol] = 0.0
                    n_filtered += 1
                    continue
                
                # Apply filtering/shrinkage
                factor = self._shrinkage_factor(float(t_stat), threshold, method)
                if factor < 1.0:
                    alphas[symbol] *= factor
                    if factor == 0.0:
                        n_filtered += 1
            
            # Log detailed metrics
            mean_after = np.mean(list(alphas.values())) if alphas else 0.0
            std_after = np.std(list(alphas.values())) if alphas else 0.0
            nz_after = sum(1 for v in alphas.values() if v != 0.0)
            
            logger.info(
                f"Alpha significance filter applied: "
                f"method={method}, threshold={threshold}, "
                f"zeroed/shrunk={n_filtered}/{n_total}, "
                f"missing_in_csv={n_missing}"
            )
            logger.info(
                f"Alpha distribution: "
                f"mean={mean_before:.6f}â†’{mean_after:.6f}, "
                f"std={std_before:.6f}â†’{std_after:.6f}, "
                f"non-zero={nz_before}â†’{nz_after}"
            )
            
        except FileNotFoundError:
            logger.warning(f"T-stat file not found: {path}. Skipping alpha significance filter.")
        except Exception as e:
            logger.error(f"Alpha significance filter failed: {e}", exc_info=True)
        
        return alphas
    
    def _shrinkage_factor(self, t_stat: float, threshold: float, method: str) -> float:
        """
        Calculate shrinkage factor based on t-statistic.
        
        Args:
            t_stat: Alpha's t-statistic
            threshold: Significance threshold (typically 2.0)
            method: Shrinkage method ('hard_threshold', 'linear_shrinkage', 'sigmoid_shrinkage')
        
        Returns:
            Shrinkage factor (0.0 to 1.0)
        """
        abs_t = abs(t_stat)
        
        if method == 'hard_threshold':
            return 1.0 if abs_t >= threshold else 0.0
        
        elif method == 'linear_shrinkage':
            # Linear decay: t=0 â†’ 0%, t=threshold â†’ 100%
            return min(1.0, abs_t / threshold)
        
        elif method == 'sigmoid_shrinkage':
            # Smooth sigmoid transition around threshold
            return 1.0 / (1.0 + np.exp(-2.0 * (abs_t - threshold)))
        
        else:
            logger.warning(f"Unknown shrinkage method: {method}, using hard_threshold")
            return 1.0 if abs_t >= threshold else 0.0

    def get_info(self) -> Dict:
        """Get Fama-French strategy information."""
        info = super().get_info()
        info.update({
            'lookback_days': self.lookback_days,
            'risk_free_rate': self.risk_free_rate,
            'strategy_type': 'fama_french_5',
            'model_complexity': 'low',
            'model_expected': 'FF5RegressionModel',
            'model_status': 'âœ… ALREADY IMPLEMENTED',
            'factor_columns': self._expected_factor_columns(),
            'data_flow': 'FactorDataProvider â†’ FF5RegressionModel',
            'prediction_optimization': 'FF5 batch prediction with factor sharing'
        })
        return info


# âœ… MODEL STATUS: IMPLEMENTED
# -----------------------------
# File: models/implementations/ff5_model.py
# Class: FF5RegressionModel(BaseModel)
#
# This model ALREADY EXISTS and implements:
# - fit(X, y): Estimates factor betas via linear regression
# - predict(X): Calculates expected returns from factor exposures
# - Uses sklearn.linear_model.LinearRegression or Ridge
#
# Model Specification:
#   R_stock = Î± + Î²_MKT*R_MKT + Î²_SMB*R_SMB + Î²_HML*R_HML + 
#             Î²_RMW*R_RMW + Î²_CMA*R_CMA + Îµ
#
# Expected Features (X):
#   - MKT: Market excess return
#   - SMB: Size factor
#   - HML: Value factor
#   - RMW: Profitability factor
#   - CMA: Investment factor
#
# Target (y):
#   - Stock excess returns (R_stock - R_f)
#
# The model is already registered and can be loaded via ModelPredictor!

